{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Final Assignment\n",
        "## CM3015 Machine Learning and Neural Networks\n",
        "\n",
        "### Credit Card Fraud Detection with a Feedforward MLP\n",
        "\n",
        "- Student: cy150\n",
        "- Workflow: Chollet's ML workflow (problem → data → evaluation → prep → baseline → model → tuning → final eval)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1 — Define the problem\n",
        "\n",
        "Chollet’s workflow keeps the project aligned with real‑world goals. It moves in clear stages from problem definition to data understanding, evaluation design, preparation, baseline, model building, tuning, and final reporting.\n",
        "\n",
        "### Overview: Credit Card Fraud\n",
        "\n",
        "Credit card fraud is the unauthorized use of a credit (or debit) card to make purchases, withdraw funds, or create transactions that the legitimate cardholder did not approve."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem Statement\n",
        "\n",
        "\n",
        "- Credit card fraud causes direct and significant financial losses: issuers, merchants, and consumers may absorb losses from unauthorized purchases and chargebacks. Fraud also creates investigative overhead (reviews, disputes) and temporary loss of funds and reputation. As a result, stricter verification and KYC are implemented by merchants and banks.\n",
        "\n",
        "-  These organizations can also lose customers if there are excessive false declines or chargebacks. Additionally, failure to protect customers according to regulatory standards can incur penalties to banks themselves proving that credit card fraud detection is a critical technology for a functioning, safe banking and finance environment\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2 — Identify and understand the data\n",
        "\n",
        "### Dataset Overview\n",
        "This dataset consists of credit card transactions by European cardholders. It covers two days of transactions with 492 frauds out of 284,807 transactions.\n",
        "The dataset is highly imbalanced, with frauds accounting for about 0.172% of all transactions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Nature of the Dataset\n",
        "\n",
        "It contains only numerical input variables resulting from a PCA transformation. Due to confidentiality, the original features and more background information are not available.\n",
        "\n",
        "### Dataset Features\n",
        "Features V1, V2, … V28 are principal components from PCA. The only features not transformed with PCA are `Time` and `Amount`.\n",
        "\n",
        "- `Time` contains the seconds elapsed between each transaction and the first transaction in the dataset.\n",
        "\n",
        "*Example*\n",
        "\n",
        "| Time |      V1 |      V2 |     V3 |     V4 |      V5 |\n",
        "| ---: | ------: | ------: | -----: | -----: | ------: |\n",
        "|    0 | -1.3598 | -0.0728 | 2.5363 | 1.3782 | -0.3383 |\n",
        "|    1 |  1.1919 |  0.2662 | 0.1665 | 0.4482 |  0.0600 |\n",
        "|    1 | -1.3584 | -1.3402 | 1.7732 | 0.3798 | -0.5032 |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset Licensing\n",
        "\n",
        "The dataset is licensed under the Database Contents License (DbCL).\n",
        "\n",
        "According to the license (Open Data Commons), the Licensor grants a worldwide, royalty-free, non-exclusive, perpetual, irrevocable copyright license to do any act that is restricted by copyright over anything within the Contents, whether in the original medium or any other. These rights explicitly include commercial use and do not exclude any field of endeavor.\n",
        "\n",
        "### Permission\n",
        "\n",
        "The DbCL license explicitly allows use of this dataset for this final assignment.\n",
        "\n",
        "### Dataset Author\n",
        "\n",
        "- Machine Learning Group - ULB "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset Source\n",
        "\n",
        "After browsing Kaggle, I selected a dataset that is complex and challenging while providing rich features for the model to learn from.\n",
        "\n",
        "Link to dataset: `https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud/data`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Justification for this dataset\n",
        "\n",
        "This dataset supports a full end-to-end deep learning workflow: clear labels, numeric features, and a real-world class imbalance that demands careful evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Rationale behind why this dataset was chosen\n",
        "\n",
        "- It matches the **problem framing**: a real-world binary classification task where the positive class (fraud) is rare and costly to miss.\n",
        "- It matches the **data modality**: fixed-length, fully numeric, tabular features that are well-suited to a feedforward MLP as a strong first baseline.\n",
        "- It forces a realistic **evaluation setup**: extreme imbalance means accuracy is not meaningful, so the workflow naturally prioritizes PR AUC, precision/recall, and explicit threshold selection.\n",
        "- It encourages good **experimental discipline**: preprocessing must be fit on the training split only (to avoid leakage) and the final test set can be kept untouched for a single, final report.\n",
        "\n",
        "#### Limitations of this dataset and mitigation strategy\n",
        "\n",
        "- **Limited interpretability**: V1–V28 are anonymized PCA components, so feature-level explanations are not meaningful.\n",
        "  - *Mitigation*: focus on predictive performance, stability across runs, and careful threshold selection rather than per-feature interpretation.\n",
        "- **Potential time effects**: `Time` reflects ordering within the 2-day window.\n",
        "  - *Mitigation*: use a time-aware split (train on earlier, validate/test on later) to better reflect deployment.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1 Cont'd — Define success metrics\n",
        "\n",
        "### Objective\n",
        "\n",
        "The primary objective is to detect fraudulent transactions while minimizing false positives. Because fraud is rare, the evaluation focuses on minority-class performance and selecting an operating point that reflects the cost of errors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3 — Choose an evaluation protocol\n",
        "\n",
        "### Holdout Protocol\n",
        "\n",
        "1. The data will be split into:\n",
        "\n",
        "    - Training set\n",
        "    - Validation set\n",
        "    - Test set\n",
        "\n",
        "   **Why**: we need separate data for learning parameters (train), choosing settings (validation), and an unbiased final report (test).\n",
        "\n",
        "   **How**: use a time-aware split when possible (earlier transactions → train, later → validation/test) so evaluation better matches deployment.\n",
        "\n",
        "2. Preprocessing decisions are fitted using the training set exclusively.\n",
        "\n",
        "   **Why**: using any information from validation/test (even feature scaling statistics) leaks signal and inflates performance.\n",
        "\n",
        "   **How**: fit transforms on `X_train` only (e.g., standardization mean/std, any imputation rules), then apply the fitted transforms unchanged to `X_val` and `X_test`.\n",
        "\n",
        "3. The validation set is only used for model and threshold selection.\n",
        "\n",
        "   **Why**: the validation set simulates unseen data during development; using it only for selection reduces the risk of overfitting the final report.\n",
        "\n",
        "   **How**: compare candidate models using PR AUC/recall/precision on validation; choose hyperparameters and pick an operating threshold (e.g., maximize recall subject to minimum precision) using validation predictions.\n",
        "\n",
        "4. Final performance is evaluated on the untouched test set.\n",
        "\n",
        "   **Why**: the test set should be a single, unbiased estimate of how the chosen pipeline will perform in the real world.\n",
        "\n",
        "   **How**: once preprocessing + model + threshold are finalized, run inference once on `X_test` and report the locked metrics (PR AUC and confusion matrix at the chosen threshold).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluation metrics\n",
        "\n",
        "This section describes how performance will be measured based on the pre-defined success criteria."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Primary Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| Metric               | What it measures                                                       | Why it matters for fraud under heavy class imbalance                                                    |\n",
        "| -------------------- | ---------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------- |\n",
        "| Precision–Recall AUC | Area under the precision–recall curve across decision thresholds       | Strong overall summary metric when fraud is rare, more informative than accuracy                        |\n",
        "| Recall               | True positive rate, how many actual fraud cases are correctly detected | Directly captures missed fraud risk since low recall means more fraud slips through                     |\n",
        "| F1 Score             | Harmonic mean of precision and recall                                  | Useful single number when you want a balanced tradeoff between catching fraud and limiting false alarms |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Secondary Metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| Secondary metric                              | Description                                                                                                       | Purpose                                                                                        |\n",
        "| --------------------------------------------- | ----------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------- |\n",
        "| Confusion matrix at a predetermined threshold | Uses TP, FP, TN, FN to make tradeoffs explicit                                                                    | Shows performance at the chosen operating point and clarifies the cost of each type of mistake |\n",
        "| Error rates                                   | False negative rate and false positive rate to quantify misses and false alarms                                   | Measures miss risk versus false alarm burden in a comparable way                               |\n",
        "| Calibration check                             | Compare predicted probabilities with observed outcomes using a simple binning table to verify probability quality | Checks whether predicted risk scores align with real observed fraud rates                      |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Justification for evaluation metrics\n",
        "\n",
        "Fraud is a rare event, so a single metric can be misleading. A model may look strong on one metric while failing in practice. The primary metric provides a consistent rule for model comparison, while secondary metrics provide the context needed to interpret false-positive and false-negative tradeoffs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Implications of the evaluation metrics\n",
        "\n",
        "1. Precision reflects workload and customer friction.\n",
        "2. Recall reflects loss prevention, often translating into direct financial losses.\n",
        "3. PR AUC reflects ranking quality under rare fraud across thresholds.\n",
        "4. The confusion matrix is reported at an operating threshold aligned with transaction behavior.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4 — Prepare the data\n",
        "\n",
        "### Data preparation plan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To prepare the data and avoid leakage:\n",
        "\n",
        "1. Use a time-aware split: earlier transactions for training, later transactions for validation/test.\n",
        "2. Fit all preprocessing steps (scaling, imputation if needed) on the training set only.\n",
        "3. Apply the same fitted transforms to validation and test sets.\n",
        "4. Preserve the class imbalance during splitting to reflect real deployment.\n",
        "5. Track feature distributions and label rate over time to identify drift.\n",
        "6. Use a small threshold sweep on the validation set for later operating-point selection.\n",
        "7. Calibrate probabilities with simple binning to sanity-check outputs.\n",
        "8. Re-train periodically as base rates drift in production."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data checks and class imbalance\n",
        "\n",
        "- Check for missing values, big outliers, and changes between the train/val/test splits.\n",
        "- Scale features using numbers from the training set only.\n",
        "- Handle imbalance with class weights or resampling.\n",
        "- Keep notes of every preprocessing step so results are repeatable.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import sys\n",
        "\n",
        "# Install into THIS kernel environment\n",
        "%pip -q install -U \"kagglehub[pandas-datasets]\"\n",
        "\n",
        "import kagglehub\n",
        "import pandas as pd\n",
        "\n",
        "print(\"Kernel Python:\", sys.version)\n",
        "print(\"Kernel executable:\", sys.executable)\n",
        "\n",
        "# Download the raw file first, then read with pandas (avoids kagglehub's UTF-8 encoding issue)\n",
        "dataset_path = kagglehub.dataset_download(\"mlg-ulb/creditcardfraud\")\n",
        "print(\"Dataset downloaded to:\", dataset_path)\n",
        "\n",
        "import os\n",
        "csv_path = os.path.join(dataset_path, \"creditcard.csv\")\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "print(f\"Shape: {df.shape}\")\n",
        "print(df.head())\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Kernel Python: 3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]\n",
            "Kernel executable: c:\\Users\\Yangu\\AppData\\Local\\Programs\\Python\\Python39\\python.exe\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.13), please consider upgrading to the latest version (0.4.2).\n",
            "Dataset downloaded to: C:\\Users\\Yangu\\.cache\\kagglehub\\datasets\\mlg-ulb\\creditcardfraud\\versions\\3\n",
            "Shape: (284807, 31)\n",
            "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
            "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
            "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
            "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
            "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
            "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
            "\n",
            "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
            "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
            "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
            "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
            "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
            "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
            "\n",
            "        V26       V27       V28  Amount  Class  \n",
            "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
            "1  0.125895 -0.008983  0.014724    2.69      0  \n",
            "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
            "3 -0.221929  0.062723  0.061458  123.50      0  \n",
            "4  0.502292  0.219422  0.215153   69.99      0  \n",
            "\n",
            "[5 rows x 31 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4 implementation (code)\n",
        "\n",
        "The code below loads the dataset from `data/creditcard.csv`.\n",
        "If the file is missing, it will try to download it using `kagglehub` and then save it to `data/creditcard.csv`.\n",
        "\n",
        "- Expected columns include `Time`, `Amount`, `V1`…`V28`, and `Class` (target label).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "DATA_PATH = Path(\"data/creditcard.csv\")\n",
        "\n",
        "# If the CSV isn't present locally, try fetching it via kagglehub.\n",
        "if not DATA_PATH.exists():\n",
        "    try:\n",
        "        import kagglehub\n",
        "\n",
        "        # Download the raw files (avoids kagglehub's internal UTF-8 read issue)\n",
        "        dataset_dir = kagglehub.dataset_download(\"mlg-ulb/creditcardfraud\")\n",
        "        src_csv = os.path.join(dataset_dir, \"creditcard.csv\")\n",
        "        df = pd.read_csv(src_csv)\n",
        "\n",
        "        # Cache locally so future runs don't re-download\n",
        "        DATA_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "        df.to_csv(DATA_PATH, index=False)\n",
        "        print(\"Downloaded via kagglehub and saved to:\", DATA_PATH)\n",
        "    except Exception as e:\n",
        "        raise FileNotFoundError(\n",
        "            f\"Could not find {DATA_PATH} and kagglehub download failed.\\n\"\n",
        "            \"Either place the file at data/creditcard.csv, or configure Kaggle access for kagglehub, then rerun.\\n\"\n",
        "            f\"Original error: {type(e).__name__}: {e}\"\n",
        "        ) from e\n",
        "else:\n",
        "    df = pd.read_csv(DATA_PATH)\n",
        "    print(\"Loaded:\", DATA_PATH)\n",
        "\n",
        "print(\"Shape:\", df.shape)\n",
        "print(df.head(3))\n",
        "\n",
        "if \"Class\" not in df.columns:\n",
        "    raise ValueError(\"Expected a 'Class' column (0=legit, 1=fraud).\")\n",
        "\n",
        "fraud_rate = df[\"Class\"].mean()\n",
        "print(f\"Fraud rate: {fraud_rate:.6f} ({df['Class'].sum()} / {len(df)})\")\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded: data\\creditcard.csv\n",
            "Shape: (284807, 31)\n",
            "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
            "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
            "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
            "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
            "\n",
            "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
            "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
            "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
            "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
            "\n",
            "        V26       V27       V28  Amount  Class  \n",
            "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
            "1  0.125895 -0.008983  0.014724    2.69      0  \n",
            "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
            "\n",
            "[3 rows x 31 columns]\n",
            "Fraud rate: 0.001727 (492 / 284807)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exploratory Data Analysis (EDA) — Visualizing the Dataset\n",
        "\n",
        "Before splitting and modelling, we explore the dataset visually. These charts serve several purposes:\n",
        "\n",
        "1. **Class distribution** — Quantify and visualize the extreme imbalance between fraud and non-fraud.\n",
        "2. **Transaction amount** — Compare the spending patterns of fraudulent vs legitimate transactions.\n",
        "3. **Transaction time** — Examine when transactions (and fraud) occur over the 2-day window.\n",
        "4. **Feature correlations** — Identify which features are most associated with the target and with each other.\n",
        "5. **Feature distributions by class** — Highlight which PCA components separate fraud from non-fraud most clearly.\n",
        "6. **Dimensionality reduction** — Use t-SNE to project all 30 features into 2D and visualize class separation.\n",
        "\n",
        "All visualizations use the full dataset before any splitting, so no leakage concerns arise at this stage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 1. Class Distribution — Bar chart + Pie chart\n",
        "# ---------------------------------------------------------------------------\n",
        "class_counts = df[\"Class\"].value_counts().sort_index()\n",
        "labels = [\"Non-fraud (0)\", \"Fraud (1)\"]\n",
        "colors = [\"#4c72b0\", \"#c44e52\"]\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
        "\n",
        "# Bar chart with log-scale option for visibility\n",
        "bars = axes[0].bar(labels, class_counts.values, color=colors, edgecolor=\"black\", linewidth=0.8)\n",
        "axes[0].set_ylabel(\"Number of Transactions\")\n",
        "axes[0].set_title(\"Class Distribution (Count)\")\n",
        "for bar, count in zip(bars, class_counts.values):\n",
        "    axes[0].text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 1500,\n",
        "                 f\"{count:,}\", ha=\"center\", va=\"bottom\", fontsize=11, fontweight=\"bold\")\n",
        "\n",
        "# Pie chart with percentage\n",
        "axes[1].pie(\n",
        "    class_counts.values,\n",
        "    labels=labels,\n",
        "    autopct=lambda pct: f\"{pct:.3f}%\\n({int(round(pct / 100 * class_counts.sum())):,})\",\n",
        "    colors=colors,\n",
        "    startangle=90,\n",
        "    explode=(0, 0.1),\n",
        "    textprops={\"fontsize\": 10},\n",
        "    wedgeprops={\"edgecolor\": \"black\", \"linewidth\": 0.5},\n",
        ")\n",
        "axes[1].set_title(\"Class Distribution (Proportion)\")\n",
        "\n",
        "plt.suptitle(\"Extreme Class Imbalance: Fraud accounts for only 0.17% of all transactions\",\n",
        "             fontsize=12, y=1.02, fontstyle=\"italic\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Non-fraud: {class_counts[0]:>8,}  ({class_counts[0]/len(df)*100:.4f}%)\")\n",
        "print(f\"Fraud:     {class_counts[1]:>8,}  ({class_counts[1]/len(df)*100:.4f}%)\")\n",
        "print(f\"Imbalance ratio: 1 fraud per {class_counts[0]//class_counts[1]:,} legitimate transactions\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---------------------------------------------------------------------------\n",
        "# 2. Transaction Amount Distribution — Fraud vs Non-fraud\n",
        "# ---------------------------------------------------------------------------\n",
        "fraud_amounts = df.loc[df[\"Class\"] == 1, \"Amount\"]\n",
        "legit_amounts = df.loc[df[\"Class\"] == 0, \"Amount\"]\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# (a) Overlapping histograms (log scale on y-axis for visibility)\n",
        "axes[0].hist(legit_amounts, bins=80, alpha=0.6, color=\"#4c72b0\",\n",
        "             label=f\"Non-fraud (n={len(legit_amounts):,})\", density=True)\n",
        "axes[0].hist(fraud_amounts, bins=80, alpha=0.7, color=\"#c44e52\",\n",
        "             label=f\"Fraud (n={len(fraud_amounts):,})\", density=True)\n",
        "axes[0].set_xlabel(\"Transaction Amount ($)\")\n",
        "axes[0].set_ylabel(\"Density\")\n",
        "axes[0].set_title(\"Amount Distribution (Overlapping)\")\n",
        "axes[0].legend(fontsize=9)\n",
        "axes[0].set_xlim(0, 500)\n",
        "\n",
        "# (b) Log-transformed amount histogram\n",
        "axes[1].hist(np.log1p(legit_amounts), bins=60, alpha=0.6, color=\"#4c72b0\",\n",
        "             label=\"Non-fraud\", density=True)\n",
        "axes[1].hist(np.log1p(fraud_amounts), bins=60, alpha=0.7, color=\"#c44e52\",\n",
        "             label=\"Fraud\", density=True)\n",
        "axes[1].set_xlabel(\"log(1 + Amount)\")\n",
        "axes[1].set_ylabel(\"Density\")\n",
        "axes[1].set_title(\"Amount Distribution (Log-Transformed)\")\n",
        "axes[1].legend(fontsize=9)\n",
        "\n",
        "# (c) Box plot comparison\n",
        "box_data = [legit_amounts.values, fraud_amounts.values]\n",
        "bp = axes[2].boxplot(box_data, labels=[\"Non-fraud\", \"Fraud\"], patch_artist=True,\n",
        "                     showfliers=False, widths=0.5)\n",
        "bp[\"boxes\"][0].set_facecolor(\"#4c72b0\")\n",
        "bp[\"boxes\"][1].set_facecolor(\"#c44e52\")\n",
        "for box in bp[\"boxes\"]:\n",
        "    box.set_alpha(0.7)\n",
        "axes[2].set_ylabel(\"Transaction Amount ($)\")\n",
        "axes[2].set_title(\"Amount Box Plot (Outliers Hidden)\")\n",
        "\n",
        "plt.suptitle(\"Fraudulent transactions tend to have different amount distributions than legitimate ones\",\n",
        "             fontsize=12, y=1.02, fontstyle=\"italic\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Summary statistics\n",
        "print(\"Amount summary statistics:\")\n",
        "print(f\"  Non-fraud — mean: ${legit_amounts.mean():.2f}, median: ${legit_amounts.median():.2f}, \"\n",
        "      f\"std: ${legit_amounts.std():.2f}, max: ${legit_amounts.max():.2f}\")\n",
        "print(f\"  Fraud     — mean: ${fraud_amounts.mean():.2f}, median: ${fraud_amounts.median():.2f}, \"\n",
        "      f\"std: ${fraud_amounts.std():.2f}, max: ${fraud_amounts.max():.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---------------------------------------------------------------------------\n",
        "# 3. Transaction Time Distribution — Fraud vs Non-fraud + Fraud rate over time\n",
        "# ---------------------------------------------------------------------------\n",
        "time_hours = df[\"Time\"] / 3600  # convert seconds to hours\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "\n",
        "# (a) Overall transaction volume over time\n",
        "axes[0, 0].hist(time_hours, bins=48, color=\"#4c72b0\", edgecolor=\"white\", alpha=0.8)\n",
        "axes[0, 0].set_xlabel(\"Time (hours since first transaction)\")\n",
        "axes[0, 0].set_ylabel(\"Number of Transactions\")\n",
        "axes[0, 0].set_title(\"Transaction Volume Over Time (All)\")\n",
        "\n",
        "# (b) Fraud vs Non-fraud time distribution\n",
        "axes[0, 1].hist(time_hours[df[\"Class\"] == 0], bins=48, alpha=0.6, color=\"#4c72b0\",\n",
        "                label=\"Non-fraud\", density=True)\n",
        "axes[0, 1].hist(time_hours[df[\"Class\"] == 1], bins=48, alpha=0.7, color=\"#c44e52\",\n",
        "                label=\"Fraud\", density=True)\n",
        "axes[0, 1].set_xlabel(\"Time (hours since first transaction)\")\n",
        "axes[0, 1].set_ylabel(\"Density\")\n",
        "axes[0, 1].set_title(\"Time Distribution by Class (Normalized)\")\n",
        "axes[0, 1].legend()\n",
        "\n",
        "# (c) Fraud rate over time (binned)\n",
        "n_bins_time = 48\n",
        "time_bins = pd.cut(time_hours, bins=n_bins_time)\n",
        "fraud_rate_by_time = df.groupby(time_bins, observed=False)[\"Class\"].mean()\n",
        "bin_centers = [interval.mid for interval in fraud_rate_by_time.index]\n",
        "\n",
        "axes[1, 0].plot(bin_centers, fraud_rate_by_time.values * 100, color=\"#c44e52\",\n",
        "                linewidth=2, marker=\"o\", markersize=3)\n",
        "axes[1, 0].fill_between(bin_centers, fraud_rate_by_time.values * 100,\n",
        "                         alpha=0.2, color=\"#c44e52\")\n",
        "axes[1, 0].set_xlabel(\"Time (hours since first transaction)\")\n",
        "axes[1, 0].set_ylabel(\"Fraud Rate (%)\")\n",
        "axes[1, 0].set_title(\"Fraud Rate Over Time\")\n",
        "axes[1, 0].axhline(y=df[\"Class\"].mean() * 100, color=\"grey\", linestyle=\"--\",\n",
        "                    label=f\"Overall fraud rate ({df['Class'].mean()*100:.3f}%)\")\n",
        "axes[1, 0].legend(fontsize=9)\n",
        "\n",
        "# (d) Cumulative fraud count over time\n",
        "fraud_times = time_hours[df[\"Class\"] == 1].sort_values()\n",
        "axes[1, 1].plot(fraud_times.values, np.arange(1, len(fraud_times) + 1),\n",
        "                color=\"#c44e52\", linewidth=2)\n",
        "axes[1, 1].set_xlabel(\"Time (hours since first transaction)\")\n",
        "axes[1, 1].set_ylabel(\"Cumulative Fraud Count\")\n",
        "axes[1, 1].set_title(\"Cumulative Fraud Transactions Over Time\")\n",
        "axes[1, 1].axhline(y=len(fraud_times), color=\"grey\", linestyle=\"--\", alpha=0.5,\n",
        "                    label=f\"Total: {len(fraud_times)} frauds\")\n",
        "axes[1, 1].legend(fontsize=9)\n",
        "\n",
        "plt.suptitle(\"Temporal patterns in transactions and fraud occurrence\",\n",
        "             fontsize=13, y=1.01, fontstyle=\"italic\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---------------------------------------------------------------------------\n",
        "# 4. Correlation Heatmap — Feature correlations with the target\n",
        "# ---------------------------------------------------------------------------\n",
        "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
        "\n",
        "# (a) Correlation of each feature with Class (target)\n",
        "corr_with_target = df.drop(columns=[\"Class\"]).corrwith(df[\"Class\"]).sort_values()\n",
        "\n",
        "bar_colors = [\"#c44e52\" if v < 0 else \"#4c72b0\" for v in corr_with_target.values]\n",
        "axes[0].barh(corr_with_target.index, corr_with_target.values, color=bar_colors,\n",
        "             edgecolor=\"white\", linewidth=0.3)\n",
        "axes[0].set_xlabel(\"Pearson Correlation with Class (Fraud)\")\n",
        "axes[0].set_title(\"Feature Correlation with Fraud Label\")\n",
        "axes[0].axvline(x=0, color=\"black\", linewidth=0.8)\n",
        "\n",
        "# Highlight the most correlated features\n",
        "for i, (feat, val) in enumerate(corr_with_target.items()):\n",
        "    if abs(val) > 0.1:\n",
        "        axes[0].text(val + 0.005 * np.sign(val), i, f\"{val:.3f}\", va=\"center\", fontsize=8)\n",
        "\n",
        "# (b) Correlation matrix heatmap of top features (most correlated with Class)\n",
        "top_features = corr_with_target.abs().nlargest(15).index.tolist()\n",
        "top_features_with_class = top_features + [\"Class\"]\n",
        "corr_matrix = df[top_features_with_class].corr()\n",
        "\n",
        "mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\n",
        "sns.heatmap(\n",
        "    corr_matrix,\n",
        "    mask=mask,\n",
        "    annot=True,\n",
        "    fmt=\".2f\",\n",
        "    cmap=\"RdBu_r\",\n",
        "    center=0,\n",
        "    vmin=-1,\n",
        "    vmax=1,\n",
        "    square=True,\n",
        "    linewidths=0.5,\n",
        "    ax=axes[1],\n",
        "    cbar_kws={\"shrink\": 0.8},\n",
        "    annot_kws={\"fontsize\": 7},\n",
        ")\n",
        "axes[1].set_title(\"Correlation Matrix (Top 15 Features + Class)\")\n",
        "\n",
        "plt.suptitle(\"Feature correlations reveal which PCA components are most associated with fraud\",\n",
        "             fontsize=12, y=1.01, fontstyle=\"italic\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print top positive and negative correlations\n",
        "print(\"Top 5 positively correlated with fraud:\")\n",
        "for feat, val in corr_with_target.nlargest(5).items():\n",
        "    print(f\"  {feat:>8s}: {val:+.4f}\")\n",
        "print(\"\\nTop 5 negatively correlated with fraud:\")\n",
        "for feat, val in corr_with_target.nsmallest(5).items():\n",
        "    print(f\"  {feat:>8s}: {val:+.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---------------------------------------------------------------------------\n",
        "# 5. Violin + Strip Plots — Top discriminative features by class\n",
        "# ---------------------------------------------------------------------------\n",
        "# Select the 8 features most correlated (positive or negative) with Class\n",
        "corr_abs = df.drop(columns=[\"Class\"]).corrwith(df[\"Class\"]).abs().sort_values(ascending=False)\n",
        "top8_features = corr_abs.head(8).index.tolist()\n",
        "\n",
        "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for i, feat in enumerate(top8_features):\n",
        "    # Subsample non-fraud for clearer violin plots (fraud is already small)\n",
        "    fraud_vals = df.loc[df[\"Class\"] == 1, feat]\n",
        "    legit_sample = df.loc[df[\"Class\"] == 0, feat].sample(n=min(2000, (df[\"Class\"] == 0).sum()),\n",
        "                                                          random_state=42)\n",
        "    plot_df = pd.DataFrame({\n",
        "        feat: pd.concat([legit_sample, fraud_vals], ignore_index=True),\n",
        "        \"Class\": [\"Non-fraud\"] * len(legit_sample) + [\"Fraud\"] * len(fraud_vals),\n",
        "    })\n",
        "\n",
        "    sns.violinplot(data=plot_df, x=\"Class\", y=feat, palette={\"Non-fraud\": \"#4c72b0\", \"Fraud\": \"#c44e52\"},\n",
        "                   inner=\"quartile\", ax=axes[i], cut=0, linewidth=0.8)\n",
        "    axes[i].set_title(f\"{feat}\\n(corr={corr_abs[feat]:.3f})\", fontsize=10)\n",
        "    axes[i].set_xlabel(\"\")\n",
        "\n",
        "plt.suptitle(\"Distribution of the 8 most discriminative features — Fraud vs Non-fraud\",\n",
        "             fontsize=13, y=1.01, fontstyle=\"italic\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---------------------------------------------------------------------------\n",
        "# 6. t-SNE 2D Projection — Visualize class separation in reduced dimensions\n",
        "# ---------------------------------------------------------------------------\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Subsample for speed: all fraud + a random sample of non-fraud\n",
        "np.random.seed(42)\n",
        "fraud_idx = df[df[\"Class\"] == 1].index.values\n",
        "legit_idx = np.random.choice(df[df[\"Class\"] == 0].index.values, size=3000, replace=False)\n",
        "sample_idx = np.concatenate([legit_idx, fraud_idx])\n",
        "np.random.shuffle(sample_idx)\n",
        "\n",
        "X_sample = df.loc[sample_idx, [c for c in df.columns if c != \"Class\"]].values\n",
        "y_sample = df.loc[sample_idx, \"Class\"].values\n",
        "\n",
        "# Run t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000, learning_rate=\"auto\")\n",
        "X_tsne = tsne.fit_transform(X_sample)\n",
        "\n",
        "# Plot\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "# Plot non-fraud first (background), then fraud on top\n",
        "mask_legit = y_sample == 0\n",
        "mask_fraud = y_sample == 1\n",
        "\n",
        "ax.scatter(X_tsne[mask_legit, 0], X_tsne[mask_legit, 1],\n",
        "           c=\"#4c72b0\", alpha=0.3, s=10, label=f\"Non-fraud (n={mask_legit.sum()})\")\n",
        "ax.scatter(X_tsne[mask_fraud, 0], X_tsne[mask_fraud, 1],\n",
        "           c=\"#c44e52\", alpha=0.8, s=25, edgecolors=\"black\", linewidth=0.5,\n",
        "           label=f\"Fraud (n={mask_fraud.sum()})\", zorder=5)\n",
        "\n",
        "ax.set_xlabel(\"t-SNE Component 1\")\n",
        "ax.set_ylabel(\"t-SNE Component 2\")\n",
        "ax.set_title(\"t-SNE 2D Projection of Transactions\\n(all fraud + 3,000 sampled non-fraud)\",\n",
        "             fontsize=13)\n",
        "ax.legend(loc=\"upper right\", fontsize=10, markerscale=2)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"t-SNE computed on {len(sample_idx):,} samples \"\n",
        "      f\"({mask_fraud.sum()} fraud + {mask_legit.sum()} non-fraud)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### EDA Summary\n",
        "\n",
        "The visualizations above reveal several key characteristics of the dataset:\n",
        "\n",
        "1. **Extreme class imbalance**: Fraud accounts for only ~0.17% of transactions. This confirms that accuracy is meaningless as a metric — a model predicting all non-fraud would achieve 99.83% accuracy while catching zero fraud.\n",
        "\n",
        "2. **Amount differences**: Fraudulent transactions tend to have different amount distributions compared to legitimate ones. The log-transformed view and box plots make these differences more visible.\n",
        "\n",
        "3. **Temporal patterns**: Transaction volume is not uniform over the 2-day window, and the fraud rate fluctuates over time. This supports the decision to use a time-aware train/validation/test split.\n",
        "\n",
        "4. **Feature correlations**: Several PCA components (e.g., V14, V17, V12, V10) show moderate-to-strong correlations with the fraud label. The correlation heatmap also confirms that PCA components are largely uncorrelated with each other (as expected from PCA), which is beneficial for modelling.\n",
        "\n",
        "5. **Violin plots**: The most discriminative features show visibly different distributions for fraud vs non-fraud, with fraud transactions having shifted or broader distributions in key components.\n",
        "\n",
        "6. **t-SNE projection**: The 2D projection shows that fraud transactions are partially separable from legitimate ones in the feature space, though there is overlap. This suggests a non-linear model like an MLP may capture boundaries that a linear model cannot.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "try:\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "except ImportError as e:\n",
        "    raise ImportError(\n",
        "        \"scikit-learn is required for StandardScaler. Install it (e.g., `pip install scikit-learn`) and rerun.\"\n",
        "    ) from e\n",
        "\n",
        "# Time-aware ordering (closer to deployment): train on earlier, test on later\n",
        "if \"Time\" in df.columns:\n",
        "    df = df.sort_values(\"Time\").reset_index(drop=True)\n",
        "\n",
        "feature_names = [c for c in df.columns if c != \"Class\"]\n",
        "X = df[feature_names].to_numpy(dtype=np.float32)\n",
        "y = df[\"Class\"].to_numpy(dtype=np.int64)\n",
        "\n",
        "n = len(df)\n",
        "train_end = int(0.70 * n)\n",
        "val_end = int(0.85 * n)\n",
        "\n",
        "X_train, y_train = X[:train_end], y[:train_end]\n",
        "X_val, y_val = X[train_end:val_end], y[train_end:val_end]\n",
        "X_test, y_test = X[val_end:], y[val_end:]\n",
        "\n",
        "print(\"Split sizes:\")\n",
        "print(\"- train:\", X_train.shape, \"fraud_rate=\", float(y_train.mean()))\n",
        "print(\"- val:  \", X_val.shape, \"fraud_rate=\", float(y_val.mean()))\n",
        "print(\"- test: \", X_test.shape, \"fraud_rate=\", float(y_test.mean()))\n",
        "\n",
        "# Fit preprocessing on TRAIN ONLY to avoid leakage\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train).astype(np.float32)\n",
        "X_val_scaled = scaler.transform(X_val).astype(np.float32)\n",
        "X_test_scaled = scaler.transform(X_test).astype(np.float32)\n",
        "\n",
        "# Optional: class weights for imbalanced training\n",
        "neg = int((y_train == 0).sum())\n",
        "pos = int((y_train == 1).sum())\n",
        "class_weight = {0: 1.0, 1: (neg / max(pos, 1))}\n",
        "print(\"class_weight:\", class_weight)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Split sizes:\n",
            "- train: (199364, 30) fraud_rate= 0.0019261250777472363\n",
            "- val:   (42721, 30) fraud_rate= 0.001310830738980829\n",
            "- test:  (42722, 30) fraud_rate= 0.0012171714807359207\n",
            "class_weight: {0: 1.0, 1: 518.1770833333334}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Split Success\n",
        "\n",
        "The split ran successfully because we created **three non-overlapping subsets** (train/val/test) with clear row counts and the same number of input features (30 columns). The sizes are sensible (about 70% / 15% / 15%), and the **fraud rate stays low and in the expected range** across all splits.\n",
        "\n",
        "The computed `class_weight` is large for class 1, which confirms fraud is rare in the training set and will be up-weighted during training. Finally, scaling is fit on `X_train` only and then applied to validation/test, which helps prevent data leakage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5 — Establish a baseline and pick a starting model\n",
        "\n",
        "In Chollet's workflow, Step 5 answers two questions: *\"What is the simplest thing that could work?\"* and *\"What is a reasonable starting architecture?\"* Before investing effort in tuning, we need reference points that tell us whether a more complex model is actually adding value.\n",
        "\n",
        "### Why baselines matter\n",
        "\n",
        "A baseline anchors the evaluation. Without one, there is no way to know whether a model's PR AUC of 0.70 is good or bad for this particular dataset and split. We establish **two** baselines of increasing complexity:\n",
        "\n",
        "1. **Trivial baseline** — always predict non-fraud. This is the \"do nothing\" model. It defines the absolute floor: any model that cannot beat it is worse than useless.\n",
        "2. **Logistic regression** — a simple linear classifier with class weighting. This is the simplest *learned* model. It tells us how much performance comes from linear relationships alone, and how much headroom remains for a non-linear model like an MLP.\n",
        "\n",
        "### Starting model choice\n",
        "\n",
        "After the baselines, we pick a starting deep-learning architecture. The choice is a **small-to-medium feedforward multilayer perceptron (MLP)** because:\n",
        "\n",
        "- The input is fixed-length, fully numeric, and tabular — ideal for Dense layers.\n",
        "- An MLP can capture non-linear interactions between PCA components that logistic regression cannot.\n",
        "- It is fast to train and simple to debug before moving to more complex architectures.\n",
        "\n",
        "The rest of this section implements these three models, evaluates them on the **validation set only**, and compares their performance side by side."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.0 — Pre-modelling data sanity check\n",
        "\n",
        "Before building any model, we verify the data is clean and confirm the assumptions from Step 4 still hold after splitting and scaling. This catches issues (NaNs introduced by scaling, duplicated rows leaking across splits, unexpected feature ranges) that would silently corrupt every model downstream."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---------------------------------------------------------------------------\n",
        "# Data sanity check before modelling\n",
        "# ---------------------------------------------------------------------------\n",
        "print(\"=== Missing values ===\")\n",
        "print(f\"  X_train NaN count: {np.isnan(X_train_scaled).sum()}\")\n",
        "print(f\"  X_val   NaN count: {np.isnan(X_val_scaled).sum()}\")\n",
        "print(f\"  X_test  NaN count: {np.isnan(X_test_scaled).sum()}\")\n",
        "\n",
        "print(\"\\n=== Infinite values ===\")\n",
        "print(f\"  X_train Inf count: {np.isinf(X_train_scaled).sum()}\")\n",
        "print(f\"  X_val   Inf count: {np.isinf(X_val_scaled).sum()}\")\n",
        "print(f\"  X_test  Inf count: {np.isinf(X_test_scaled).sum()}\")\n",
        "\n",
        "print(\"\\n=== Duplicate rows in original df ===\")\n",
        "n_dup = df.duplicated().sum()\n",
        "print(f\"  Total duplicates: {n_dup} ({n_dup / len(df) * 100:.2f}%)\")\n",
        "\n",
        "print(\"\\n=== Feature range after scaling (train) ===\")\n",
        "train_df_scaled = pd.DataFrame(X_train_scaled, columns=feature_names)\n",
        "print(train_df_scaled.describe().loc[[\"mean\", \"std\", \"min\", \"max\"]].round(2).to_string())\n",
        "\n",
        "print(\"\\n=== Label counts per split ===\")\n",
        "for name, labels in [(\"train\", y_train), (\"val\", y_val), (\"test\", y_test)]:\n",
        "    unique, counts = np.unique(labels, return_counts=True)\n",
        "    print(f\"  {name:5s}: \" + \", \".join(f\"class {u}={c}\" for u, c in zip(unique, counts)))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=== Missing values ===\n",
            "  X_train NaN count: 0\n",
            "  X_val   NaN count: 0\n",
            "  X_test  NaN count: 0\n",
            "\n",
            "=== Infinite values ===\n",
            "  X_train Inf count: 0\n",
            "  X_val   Inf count: 0\n",
            "  X_test  Inf count: 0\n",
            "\n",
            "=== Duplicate rows in original df ===\n",
            "  Total duplicates: 1081 (0.38%)\n",
            "\n",
            "=== Feature range after scaling (train) ===\n",
            "      Time     V1     V2     V3     V4     V5     V6     V7     V8     V9    V10    V11    V12   V13    V14   V15    V16    V17    V18   V19    V20    V21    V22    V23   V24    V25   V26    V27     V28  Amount\n",
            "mean  0.00   0.00   0.00   0.00   0.00  -0.00   0.00  -0.00  -0.00   0.00  -0.00  -0.00   0.00  0.00  -0.00 -0.00   0.00   0.00  -0.00 -0.00   0.00   0.00   0.00  -0.00  0.00  -0.00 -0.00  -0.00    0.00   -0.00\n",
            "std   1.00   1.00   1.00   1.00   1.00   1.00   1.00   1.00   1.00   1.00   1.00   1.00   1.00  1.00   1.00  1.00   1.00   1.00   1.00  1.00   1.00   1.00   1.00   1.00  1.00   1.00  1.00   1.00    1.00    1.00\n",
            "min  -2.04 -29.77 -44.84 -23.49  -4.12 -30.87 -19.99 -35.72 -60.54 -11.84 -22.49  -4.73 -17.01 -5.65 -19.82 -4.85 -15.94 -28.28 -11.27 -8.79 -33.69 -47.16 -15.75 -73.34 -4.71 -21.08 -5.35 -57.27  -37.32   -0.36\n",
            "max   1.81   1.35  13.60   6.26  12.01  25.64  17.15  30.15  16.52  13.72  21.74  11.43   7.24  4.39  10.78  6.03   6.89  10.36   6.05  6.84  52.59  36.87  15.26  31.14  6.66  15.18  7.19  30.84  107.84   78.61\n",
            "\n",
            "=== Label counts per split ===\n",
            "  train: class 0=198980, class 1=384\n",
            "  val  : class 0=42665, class 1=56\n",
            "  test : class 0=42670, class 1=52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.0.1 — Post-split visual sanity checks\n",
        "\n",
        "The numerical sanity check above confirmed there are no NaNs, Infs, or severe anomalies. The following charts provide a **visual** verification that:\n",
        "\n",
        "1. The splits have sensible class distributions (no accidental stratification issues).\n",
        "2. Feature distributions are similar across train/val/test (no unexpected drift from the time-aware split).\n",
        "3. The Amount and Time distributions remain consistent across splits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---------------------------------------------------------------------------\n",
        "# Post-split visual checks\n",
        "# ---------------------------------------------------------------------------\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "\n",
        "# --- Row 1: Split-level class distribution ---\n",
        "split_names = [\"Train\", \"Validation\", \"Test\"]\n",
        "split_labels = [y_train, y_val, y_test]\n",
        "split_sizes = [len(y_train), len(y_val), len(y_test)]\n",
        "\n",
        "# (a) Bar chart of split sizes\n",
        "bars = axes[0, 0].bar(split_names, split_sizes, color=[\"#4c72b0\", \"#55a868\", \"#c44e52\"],\n",
        "                       edgecolor=\"black\", linewidth=0.8)\n",
        "for bar, sz in zip(bars, split_sizes):\n",
        "    axes[0, 0].text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 500,\n",
        "                     f\"{sz:,}\", ha=\"center\", va=\"bottom\", fontsize=10, fontweight=\"bold\")\n",
        "axes[0, 0].set_ylabel(\"Number of Samples\")\n",
        "axes[0, 0].set_title(\"Split Sizes (70/15/15)\")\n",
        "\n",
        "# (b) Fraud rate per split\n",
        "fraud_rates = [y.mean() * 100 for y in split_labels]\n",
        "bar_colors = [\"#4c72b0\", \"#55a868\", \"#c44e52\"]\n",
        "bars_fr = axes[0, 1].bar(split_names, fraud_rates, color=bar_colors, edgecolor=\"black\", linewidth=0.8)\n",
        "for bar, fr in zip(bars_fr, fraud_rates):\n",
        "    axes[0, 1].text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01,\n",
        "                     f\"{fr:.4f}%\", ha=\"center\", va=\"bottom\", fontsize=10)\n",
        "axes[0, 1].set_ylabel(\"Fraud Rate (%)\")\n",
        "axes[0, 1].set_title(\"Fraud Rate per Split\")\n",
        "axes[0, 1].axhline(y=df[\"Class\"].mean() * 100, color=\"grey\", linestyle=\"--\",\n",
        "                    label=f\"Overall: {df['Class'].mean()*100:.4f}%\", linewidth=1)\n",
        "axes[0, 1].legend(fontsize=9)\n",
        "\n",
        "# (c) Fraud count per split (stacked)\n",
        "fraud_counts = [y.sum() for y in split_labels]\n",
        "legit_counts = [len(y) - y.sum() for y in split_labels]\n",
        "axes[0, 2].bar(split_names, legit_counts, color=\"#4c72b0\", label=\"Non-fraud\", edgecolor=\"white\")\n",
        "axes[0, 2].bar(split_names, fraud_counts, bottom=legit_counts, color=\"#c44e52\",\n",
        "               label=\"Fraud\", edgecolor=\"white\")\n",
        "axes[0, 2].set_ylabel(\"Count\")\n",
        "axes[0, 2].set_title(\"Class Counts per Split\")\n",
        "axes[0, 2].legend()\n",
        "for i, (fc, name) in enumerate(zip(fraud_counts, split_names)):\n",
        "    axes[0, 2].text(i, legit_counts[i] + fc + 500, f\"Fraud: {fc}\", ha=\"center\", fontsize=9)\n",
        "\n",
        "# --- Row 2: Feature distributions across splits (scaled) ---\n",
        "# Amount distribution across splits\n",
        "amount_idx = feature_names.index(\"Amount\")\n",
        "axes[1, 0].hist(X_train_scaled[:, amount_idx], bins=60, alpha=0.5, color=\"#4c72b0\",\n",
        "                label=\"Train\", density=True)\n",
        "axes[1, 0].hist(X_val_scaled[:, amount_idx], bins=60, alpha=0.5, color=\"#55a868\",\n",
        "                label=\"Val\", density=True)\n",
        "axes[1, 0].hist(X_test_scaled[:, amount_idx], bins=60, alpha=0.5, color=\"#c44e52\",\n",
        "                label=\"Test\", density=True)\n",
        "axes[1, 0].set_xlabel(\"Scaled Amount\")\n",
        "axes[1, 0].set_ylabel(\"Density\")\n",
        "axes[1, 0].set_title(\"Amount Distribution Across Splits (Scaled)\")\n",
        "axes[1, 0].legend(fontsize=9)\n",
        "axes[1, 0].set_xlim(-2, 10)\n",
        "\n",
        "# Time distribution across splits\n",
        "time_idx = feature_names.index(\"Time\")\n",
        "axes[1, 1].hist(X_train_scaled[:, time_idx], bins=60, alpha=0.5, color=\"#4c72b0\",\n",
        "                label=\"Train\", density=True)\n",
        "axes[1, 1].hist(X_val_scaled[:, time_idx], bins=60, alpha=0.5, color=\"#55a868\",\n",
        "                label=\"Val\", density=True)\n",
        "axes[1, 1].hist(X_test_scaled[:, time_idx], bins=60, alpha=0.5, color=\"#c44e52\",\n",
        "                label=\"Test\", density=True)\n",
        "axes[1, 1].set_xlabel(\"Scaled Time\")\n",
        "axes[1, 1].set_ylabel(\"Density\")\n",
        "axes[1, 1].set_title(\"Time Distribution Across Splits (Scaled)\")\n",
        "axes[1, 1].legend(fontsize=9)\n",
        "\n",
        "# V14 distribution across splits (one of the most discriminative features)\n",
        "v14_idx = feature_names.index(\"V14\")\n",
        "axes[1, 2].hist(X_train_scaled[:, v14_idx], bins=60, alpha=0.5, color=\"#4c72b0\",\n",
        "                label=\"Train\", density=True)\n",
        "axes[1, 2].hist(X_val_scaled[:, v14_idx], bins=60, alpha=0.5, color=\"#55a868\",\n",
        "                label=\"Val\", density=True)\n",
        "axes[1, 2].hist(X_test_scaled[:, v14_idx], bins=60, alpha=0.5, color=\"#c44e52\",\n",
        "                label=\"Test\", density=True)\n",
        "axes[1, 2].set_xlabel(\"Scaled V14\")\n",
        "axes[1, 2].set_ylabel(\"Density\")\n",
        "axes[1, 2].set_title(\"V14 Distribution Across Splits (Scaled)\")\n",
        "axes[1, 2].legend(fontsize=9)\n",
        "\n",
        "plt.suptitle(\"Post-split verification: split sizes, fraud rates, and feature distributions are consistent\",\n",
        "             fontsize=12, y=1.01, fontstyle=\"italic\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 — Trivial baseline: always predict non-fraud\n",
        "\n",
        "The simplest possible model predicts **class 0** (non-fraud) for every transaction, regardless of its features. This is equivalent to a system that never flags anything.\n",
        "\n",
        "**What to expect:**\n",
        "\n",
        "| Metric | Expected value | Why |\n",
        "| --- | --- | --- |\n",
        "| Recall | 0 | The model never predicts fraud, so it catches zero fraud cases (all frauds are false negatives). |\n",
        "| Precision | 0 | There are no positive predictions, so precision is undefined and reported as 0. |\n",
        "| F1 | 0 | Harmonic mean of two zeros. |\n",
        "| PR AUC | ~ fraud prevalence (~0.13%) | With constant scores, precision-recall collapses to the positive-class base rate. |\n",
        "\n",
        "**Why this matters:** In practice, a model with 99.87% accuracy (by predicting all non-fraud) sounds impressive but is completely useless — it misses every single fraud. This demonstrates why accuracy is not a meaningful metric under heavy class imbalance, and why we use PR AUC, recall, and F1 instead.\n",
        "\n",
        "The cell below computes these metrics on the **validation set** and prints the full confusion matrix so we can see exactly how many fraud cases are missed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.metrics import (\n",
        "    precision_recall_curve,\n",
        "    auc,\n",
        "    f1_score,\n",
        "    recall_score,\n",
        "    precision_score,\n",
        "    confusion_matrix,\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Trivial baseline: predict class 0 (non-fraud) for every sample\n",
        "# ---------------------------------------------------------------------------\n",
        "y_val_pred_trivial = np.zeros_like(y_val)          # hard predictions (all 0)\n",
        "y_val_prob_trivial = np.zeros(len(y_val))           # probability scores (all 0.0)\n",
        "\n",
        "# Metrics at the trivial decision\n",
        "print(\"=== Trivial Baseline (always predict non-fraud) ===\")\n",
        "print(f\"Recall:    {recall_score(y_val, y_val_pred_trivial):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_val, y_val_pred_trivial, zero_division=0):.4f}\")\n",
        "print(f\"F1 Score:  {f1_score(y_val, y_val_pred_trivial):.4f}\")\n",
        "\n",
        "# PR AUC — for a constant predictor, precision = fraud_rate at all recall levels\n",
        "prec_t, rec_t, _ = precision_recall_curve(y_val, y_val_prob_trivial)\n",
        "pr_auc_trivial = auc(rec_t, prec_t)\n",
        "print(f\"PR AUC:    {pr_auc_trivial:.4f}\")\n",
        "\n",
        "print(\"\\nConfusion matrix (rows=actual, cols=predicted):\")\n",
        "cm = confusion_matrix(y_val, y_val_pred_trivial)\n",
        "print(cm)\n",
        "print(f\"\\n  TN={cm[0,0]}  FP={cm[0,1]}\")\n",
        "print(f\"  FN={cm[1,0]}  TP={cm[1,1]}\")\n",
        "print(f\"\\n  False-negative rate: {cm[1,0] / max(cm[1].sum(), 1):.4f}\")\n",
        "print(f\"  False-positive rate: {cm[0,1] / max(cm[0].sum(), 1):.4f}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=== Trivial Baseline (always predict non-fraud) ===\n",
            "Recall:    0.0000\n",
            "Precision: 0.0000\n",
            "F1 Score:  0.0000\n",
            "PR AUC:    0.5007\n",
            "\n",
            "Confusion matrix (rows=actual, cols=predicted):\n",
            "[[42665     0]\n",
            " [   56     0]]\n",
            "\n",
            "  TN=42665  FP=0\n",
            "  FN=56  TP=0\n",
            "\n",
            "  False-negative rate: 1.0000\n",
            "  False-positive rate: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 — Logistic regression baseline\n",
        "\n",
        "Logistic regression is a classical linear classifier that models the log-odds of the positive class as a linear combination of input features. Despite its simplicity, it is a strong first baseline for tabular data.\n",
        "\n",
        "**Key configuration choices:**\n",
        "- **`class_weight=\"balanced\"`**: scikit-learn automatically computes weights inversely proportional to class frequencies. This means each fraud sample contributes far more to the loss than each legitimate sample, forcing the model to pay attention to the minority class without requiring manual weight calculation.\n",
        "- **`solver=\"lbfgs\"`**: a quasi-Newton optimization method well-suited for small-to-medium datasets with L2 regularization.\n",
        "- **`max_iter=1000`**: ensures the optimizer has enough iterations to converge on this dataset.\n",
        "- **`random_state=42`**: fixes the random seed for reproducibility.\n",
        "\n",
        "**What this baseline tells us:**\n",
        "- It reveals how much of the fraud signal lives in **linear** combinations of the 30 input features (Time, Amount, V1–V28).\n",
        "- It provides **calibrated probability scores** out of the box, meaning the predicted probabilities are already roughly aligned with true fraud rates — useful as a calibration reference.\n",
        "- The gap between logistic regression and the MLP later will show us how much the non-linear layers contribute.\n",
        "\n",
        "**Evaluation approach:** We use the default threshold of 0.5 for hard predictions and compute the same metrics as the trivial baseline (recall, precision, F1, PR AUC, confusion matrix) so the comparison is apples-to-apples. The threshold will be optimized in Step 7."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Logistic regression baseline (with class weighting for imbalance)\n",
        "# ---------------------------------------------------------------------------\n",
        "lr = LogisticRegression(\n",
        "    class_weight=\"balanced\",   # auto-weight inversely proportional to class freq\n",
        "    max_iter=1000,\n",
        "    solver=\"lbfgs\",\n",
        "    random_state=42,\n",
        ")\n",
        "lr.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predicted probabilities on the validation set\n",
        "y_val_prob_lr = lr.predict_proba(X_val_scaled)[:, 1]\n",
        "\n",
        "# Use a 0.5 default threshold for hard predictions\n",
        "y_val_pred_lr = (y_val_prob_lr >= 0.5).astype(int)\n",
        "\n",
        "print(\"=== Logistic Regression Baseline ===\")\n",
        "print(f\"Recall:    {recall_score(y_val, y_val_pred_lr):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_val, y_val_pred_lr, zero_division=0):.4f}\")\n",
        "print(f\"F1 Score:  {f1_score(y_val, y_val_pred_lr):.4f}\")\n",
        "\n",
        "prec_lr, rec_lr, _ = precision_recall_curve(y_val, y_val_prob_lr)\n",
        "pr_auc_lr = auc(rec_lr, prec_lr)\n",
        "print(f\"PR AUC:    {pr_auc_lr:.4f}\")\n",
        "\n",
        "print(\"\\nConfusion matrix (rows=actual, cols=predicted):\")\n",
        "cm_lr = confusion_matrix(y_val, y_val_pred_lr)\n",
        "print(cm_lr)\n",
        "print(f\"\\n  TN={cm_lr[0,0]}  FP={cm_lr[0,1]}\")\n",
        "print(f\"  FN={cm_lr[1,0]}  TP={cm_lr[1,1]}\")\n",
        "print(f\"\\n  False-negative rate: {cm_lr[1,0] / max(cm_lr[1].sum(), 1):.4f}\")\n",
        "print(f\"  False-positive rate: {cm_lr[0,1] / max(cm_lr[0].sum(), 1):.4f}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=== Logistic Regression Baseline ===\n",
            "Recall:    0.9286\n",
            "Precision: 0.0530\n",
            "F1 Score:  0.1002\n",
            "PR AUC:    0.8389\n",
            "\n",
            "Confusion matrix (rows=actual, cols=predicted):\n",
            "[[41735   930]\n",
            " [    4    52]]\n",
            "\n",
            "  TN=41735  FP=930\n",
            "  FN=4  TP=52\n",
            "\n",
            "  False-negative rate: 0.0714\n",
            "  False-positive rate: 0.0218\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3 — Starting model: feedforward MLP\n",
        "\n",
        "The MLP is the starting deep-learning model. It builds on the logistic regression baseline by adding **non-linear hidden layers** that can capture interactions and complex decision boundaries that a linear model cannot.\n",
        "\n",
        "#### Architecture overview\n",
        "\n",
        "The network takes 30 input features and passes them through three progressively narrower hidden layers before producing a single fraud probability:\n",
        "\n",
        "```\n",
        "Input (30) → Dense(128) → Dropout(0.4) → Dense(64) → Dropout(0.3) → Dense(32) → Dropout(0.3) → Dense(1, sigmoid)\n",
        "```\n",
        "\n",
        "#### Design decisions explained\n",
        "\n",
        "| Component | Choice | Rationale |\n",
        "|---|---|---|\n",
        "| **Hidden layers** | 3 Dense layers (128 → 64 → 32) | A \"funnel\" shape that progressively compresses information. 128 units in the first layer gives enough capacity to capture feature interactions; narrowing to 32 forces the network to distill the most discriminative patterns. Three layers is a moderate depth — enough for non-linearity without excessive risk of overfitting on ~200k training samples. |\n",
        "| **Activation** | ReLU (Rectified Linear Unit) | Standard default for hidden layers. ReLU outputs `max(0, x)`, which avoids the vanishing gradient problem that plagues sigmoid/tanh in deep networks, and is computationally cheap. |\n",
        "| **Regularization** | Dropout (0.4 on first layer, 0.3 on others) | During each training step, Dropout randomly sets a fraction of neuron outputs to zero. This prevents neurons from co-adapting (relying on specific other neurons) and acts as an ensemble of thinned networks. The first layer gets higher dropout (0.4) because it has the most parameters. |\n",
        "| **Output layer** | 1 unit with sigmoid activation | Sigmoid squashes the output to [0, 1], directly interpretable as a fraud probability. A single unit is standard for binary classification. |\n",
        "| **Loss function** | Binary cross-entropy with class weights | Cross-entropy measures how well the predicted probabilities match the true labels. The `class_weight` dictionary (computed in Step 4) multiplies the loss for fraud samples by ~518×, ensuring the network does not simply learn to predict all-zero. |\n",
        "| **Optimizer** | Adam (learning rate = 1e-3) | Adam combines momentum and adaptive per-parameter learning rates. The default lr of 1e-3 is a well-tested starting point for tabular data. |\n",
        "| **Early stopping** | Monitor `val_loss`, patience = 10, restore best weights | Training stops if validation loss does not improve for 10 consecutive epochs, and the model reverts to the weights from the best epoch. This prevents overfitting without requiring manual epoch selection. |\n",
        "\n",
        "#### Why this specific size?\n",
        "\n",
        "- **Too small** (e.g., a single 16-unit layer) may underfit: the model cannot capture enough non-linear structure.\n",
        "- **Too large** (e.g., 512 → 256 → 128) risks overfitting on a dataset where only ~384 fraud cases exist in the training set.\n",
        "- The 128 → 64 → 32 configuration is a deliberate middle ground, and Step 7 will experiment with variations.\n",
        "\n",
        "The cell below defines the `build_mlp` function, instantiates the model, compiles it, and prints the architecture summary (layer types, output shapes, and parameter counts)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import random, os\n",
        "import tensorflow as tf\n",
        "\n",
        "# Use attribute access (more reliable across TF versions than 'from tensorflow.keras import ...')\n",
        "keras = tf.keras\n",
        "layers = tf.keras.layers\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Reproducibility: set all random seeds\n",
        "# ---------------------------------------------------------------------------\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Build the starting MLP\n",
        "# ---------------------------------------------------------------------------\n",
        "def build_mlp(input_dim: int, name: str = \"fraud_mlp\") -> keras.Model:\n",
        "    \"\"\"Small-to-medium feedforward MLP for binary fraud classification.\"\"\"\n",
        "    model = keras.Sequential(\n",
        "        [\n",
        "            layers.Input(shape=(input_dim,)),\n",
        "            layers.Dense(128, activation=\"relu\"),\n",
        "            layers.Dropout(0.4),\n",
        "            layers.Dense(64, activation=\"relu\"),\n",
        "            layers.Dropout(0.3),\n",
        "            layers.Dense(32, activation=\"relu\"),\n",
        "            layers.Dropout(0.3),\n",
        "            layers.Dense(1, activation=\"sigmoid\"),\n",
        "        ],\n",
        "        name=name,\n",
        "    )\n",
        "    return model\n",
        "\n",
        "input_dim = X_train_scaled.shape[1]\n",
        "mlp = build_mlp(input_dim)\n",
        "\n",
        "mlp.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics=[\"AUC\"],       # ROC-AUC tracked during training for quick reference\n",
        ")\n",
        "\n",
        "mlp.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"fraud_mlp\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 128)               3968      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 32)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,337\n",
            "Trainable params: 14,337\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.4 — Train the starting MLP\n",
        "\n",
        "This cell runs the actual training loop. Here is what happens under the hood at each epoch:\n",
        "\n",
        "1. **Forward pass**: the training data is fed through the network in mini-batches of 2,048 samples. For each sample, the network produces a fraud probability.\n",
        "2. **Loss computation**: binary cross-entropy is calculated between the predicted probabilities and the true labels. Each fraud sample's loss is multiplied by the `class_weight[1]` (~518×) so that missing a fraud case is penalized far more heavily than a false alarm.\n",
        "3. **Backward pass**: gradients of the loss with respect to every weight are computed via backpropagation.\n",
        "4. **Weight update**: the Adam optimizer uses these gradients (plus its momentum and adaptive learning rate state) to update the network's weights.\n",
        "5. **Validation check**: after each epoch, the model is evaluated on the validation set (without Dropout) and `val_loss` is recorded.\n",
        "\n",
        "**Early stopping** watches `val_loss`:\n",
        "- If `val_loss` improves (decreases), the current weights are saved internally as the \"best so far.\"\n",
        "- If `val_loss` does not improve for 10 consecutive epochs (`patience=10`), training halts early and the model's weights are **reverted** to the best checkpoint. This means the final model is not the one from the last epoch, but from the epoch with the lowest validation loss.\n",
        "\n",
        "**Batch size = 2,048**: a relatively large batch for this dataset size. Larger batches give more stable gradient estimates per step and train faster on GPU, but can sometimes converge to sharper minima. This is a reasonable default for ~200k training samples.\n",
        "\n",
        "**Max epochs = 100**: an upper bound. In practice, early stopping typically triggers well before 100 epochs.\n",
        "\n",
        "The training cell is followed by a plotting cell that visualizes the loss and ROC-AUC curves over epochs, so you can visually confirm that (a) training loss decreases, (b) validation loss eventually plateaus or rises (triggering early stopping), and (c) there is no severe divergence between train and val curves (which would indicate overfitting)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---------------------------------------------------------------------------\n",
        "# Train the MLP with early stopping\n",
        "# ---------------------------------------------------------------------------\n",
        "early_stop = keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    patience=10,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "history = mlp.fit(\n",
        "    X_train_scaled,\n",
        "    y_train,\n",
        "    validation_data=(X_val_scaled, y_val),\n",
        "    epochs=100,\n",
        "    batch_size=2048,\n",
        "    class_weight=class_weight,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.7948 - auc: 0.9023 - val_loss: 0.1582 - val_auc: 0.9615\n",
            "Epoch 2/100\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 0.4495 - auc: 0.9597 - val_loss: 0.0975 - val_auc: 0.9691\n",
            "Epoch 3/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.3944 - auc: 0.9676 - val_loss: 0.1064 - val_auc: 0.9786\n",
            "Epoch 4/100\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 0.3333 - auc: 0.9757 - val_loss: 0.0662 - val_auc: 0.9804\n",
            "Epoch 5/100\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 0.2964 - auc: 0.9836 - val_loss: 0.0652 - val_auc: 0.9800\n",
            "Epoch 6/100\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 0.3101 - auc: 0.9813 - val_loss: 0.0646 - val_auc: 0.9808\n",
            "Epoch 7/100\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 0.2548 - auc: 0.9880 - val_loss: 0.0419 - val_auc: 0.9820\n",
            "Epoch 8/100\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 0.2849 - auc: 0.9838 - val_loss: 0.0590 - val_auc: 0.9822\n",
            "Epoch 9/100\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 0.2202 - auc: 0.9921 - val_loss: 0.0475 - val_auc: 0.9814\n",
            "Epoch 10/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.2443 - auc: 0.9896 - val_loss: 0.0381 - val_auc: 0.9753\n",
            "Epoch 11/100\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 0.2175 - auc: 0.9910 - val_loss: 0.0394 - val_auc: 0.9753\n",
            "Epoch 12/100\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 0.2125 - auc: 0.9923 - val_loss: 0.0326 - val_auc: 0.9694\n",
            "Epoch 13/100\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 0.2051 - auc: 0.9916 - val_loss: 0.0344 - val_auc: 0.9697\n",
            "Epoch 14/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.1991 - auc: 0.9934 - val_loss: 0.0265 - val_auc: 0.9708\n",
            "Epoch 15/100\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 0.2056 - auc: 0.9938 - val_loss: 0.0310 - val_auc: 0.9634\n",
            "Epoch 16/100\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 0.1970 - auc: 0.9930 - val_loss: 0.0188 - val_auc: 0.9652\n",
            "Epoch 17/100\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 0.1769 - auc: 0.9954 - val_loss: 0.0300 - val_auc: 0.9643\n",
            "Epoch 18/100\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 0.1516 - auc: 0.9964 - val_loss: 0.0215 - val_auc: 0.9588\n",
            "Epoch 19/100\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 0.1822 - auc: 0.9947 - val_loss: 0.0271 - val_auc: 0.9660\n",
            "Epoch 20/100\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 0.1804 - auc: 0.9937 - val_loss: 0.0269 - val_auc: 0.9660\n",
            "Epoch 21/100\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 0.1407 - auc: 0.9969 - val_loss: 0.0157 - val_auc: 0.9606\n",
            "Epoch 22/100\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 0.1541 - auc: 0.9962 - val_loss: 0.0172 - val_auc: 0.9603\n",
            "Epoch 23/100\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 0.1332 - auc: 0.9974 - val_loss: 0.0163 - val_auc: 0.9609\n",
            "Epoch 24/100\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 0.1322 - auc: 0.9974 - val_loss: 0.0172 - val_auc: 0.9608\n",
            "Epoch 25/100\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 0.1359 - auc: 0.9970 - val_loss: 0.0157 - val_auc: 0.9611\n",
            "Epoch 26/100\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 0.1556 - auc: 0.9954 - val_loss: 0.0159 - val_auc: 0.9606\n",
            "Epoch 27/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.1168 - auc: 0.9980 - val_loss: 0.0144 - val_auc: 0.9613\n",
            "Epoch 28/100\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 0.1278 - auc: 0.9975 - val_loss: 0.0167 - val_auc: 0.9609\n",
            "Epoch 29/100\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.1205 - auc: 0.9978 - val_loss: 0.0159 - val_auc: 0.9610\n",
            "Epoch 30/100\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 0.1110 - auc: 0.9980 - val_loss: 0.0150 - val_auc: 0.9614\n",
            "Epoch 31/100\n",
            "38/98 [==========>...................] - ETA: 0s - loss: 0.1143 - auc: 0.9978"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Plot training curves\n",
        "# ---------------------------------------------------------------------------\n",
        "# Detect the AUC key name (varies across TensorFlow versions: \"auc\", \"auc_1\", etc.)\n",
        "auc_key = [k for k in history.history if k.startswith(\"auc\") and not k.startswith(\"val_\")][0]\n",
        "val_auc_key = f\"val_{auc_key}\"\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "axes[0].plot(history.history[\"loss\"], label=\"train loss\")\n",
        "axes[0].plot(history.history[\"val_loss\"], label=\"val loss\")\n",
        "axes[0].set_title(\"Binary Cross-Entropy Loss\")\n",
        "axes[0].set_xlabel(\"Epoch\")\n",
        "axes[0].set_ylabel(\"Loss\")\n",
        "axes[0].legend()\n",
        "\n",
        "axes[1].plot(history.history[auc_key], label=\"train AUC\")\n",
        "axes[1].plot(history.history[val_auc_key], label=\"val AUC\")\n",
        "axes[1].set_title(\"ROC AUC\")\n",
        "axes[1].set_xlabel(\"Epoch\")\n",
        "axes[1].set_ylabel(\"AUC\")\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "best_epoch = np.argmin(history.history[\"val_loss\"])\n",
        "print(f\"Best epoch (lowest val_loss): {best_epoch + 1}\")\n",
        "print(f\"  train loss: {history.history['loss'][best_epoch]:.4f}  |  val loss: {history.history['val_loss'][best_epoch]:.4f}\")\n",
        "print(f\"  train AUC:  {history.history[auc_key][best_epoch]:.4f}  |  val AUC:  {history.history[val_auc_key][best_epoch]:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.5 — Evaluate the MLP on the validation set and compare with baselines\n",
        "\n",
        "Now that all three models are ready, this section evaluates them under identical conditions so the comparison is fair.\n",
        "\n",
        "**How evaluation works:**\n",
        "1. The trained MLP runs inference on `X_val_scaled` (the scaled validation features). The output is a vector of fraud probabilities, one per transaction.\n",
        "2. A **default threshold of 0.5** converts probabilities into hard predictions: if the model says ≥ 0.5, predict fraud; otherwise, predict non-fraud. (This threshold is deliberately not optimized yet — Step 7 will tune it on the validation set.)\n",
        "3. The same four metrics are computed for the MLP as for the two baselines:\n",
        "   - **Recall** — what fraction of actual frauds did the model catch?\n",
        "   - **Precision** — of the transactions the model flagged, how many were truly fraud?\n",
        "   - **F1** — the harmonic mean of precision and recall, giving a single balanced number.\n",
        "   - **PR AUC** — the area under the precision–recall curve, summarizing performance across *all* possible thresholds (not just 0.5).\n",
        "\n",
        "**The comparison table** collects all four metrics for the three models in a single DataFrame, with the best value in each column highlighted in green. This makes it immediately clear which model leads on each metric.\n",
        "\n",
        "**The PR curve overlay** plots the full precision–recall trade-off for all three models on the same axes:\n",
        "- The **trivial baseline** appears as a flat line near the bottom — it cannot trade off precision for recall because it never predicts positive.\n",
        "- The **logistic regression** curve shows how a linear model ranks transactions by fraud risk.\n",
        "- The **MLP** curve shows the improvement (or lack thereof) from non-linear layers.\n",
        "\n",
        "A model whose PR curve is higher and further to the right dominates: it achieves higher precision at every recall level. The area under each curve (PR AUC) is the single-number summary of this ranking quality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---------------------------------------------------------------------------\n",
        "# MLP validation metrics\n",
        "# ---------------------------------------------------------------------------\n",
        "y_val_prob_mlp = mlp.predict(X_val_scaled, verbose=0).ravel()\n",
        "y_val_pred_mlp = (y_val_prob_mlp >= 0.5).astype(int)\n",
        "\n",
        "print(\"=== Starting MLP (threshold = 0.5) ===\")\n",
        "print(f\"Recall:    {recall_score(y_val, y_val_pred_mlp):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_val, y_val_pred_mlp, zero_division=0):.4f}\")\n",
        "print(f\"F1 Score:  {f1_score(y_val, y_val_pred_mlp):.4f}\")\n",
        "\n",
        "prec_mlp, rec_mlp, _ = precision_recall_curve(y_val, y_val_prob_mlp)\n",
        "pr_auc_mlp = auc(rec_mlp, prec_mlp)\n",
        "print(f\"PR AUC:    {pr_auc_mlp:.4f}\")\n",
        "\n",
        "print(\"\\nConfusion matrix (rows=actual, cols=predicted):\")\n",
        "cm_mlp = confusion_matrix(y_val, y_val_pred_mlp)\n",
        "print(cm_mlp)\n",
        "print(f\"\\n  TN={cm_mlp[0,0]}  FP={cm_mlp[0,1]}\")\n",
        "print(f\"  FN={cm_mlp[1,0]}  TP={cm_mlp[1,1]}\")\n",
        "print(f\"\\n  False-negative rate: {cm_mlp[1,0] / max(cm_mlp[1].sum(), 1):.4f}\")\n",
        "print(f\"  False-positive rate: {cm_mlp[0,1] / max(cm_mlp[0].sum(), 1):.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---------------------------------------------------------------------------\n",
        "# Side-by-side comparison table\n",
        "# ---------------------------------------------------------------------------\n",
        "comparison = pd.DataFrame(\n",
        "    {\n",
        "        \"Model\": [\"Trivial (all non-fraud)\", \"Logistic Regression\", \"Starting MLP\"],\n",
        "        \"PR AUC\": [pr_auc_trivial, pr_auc_lr, pr_auc_mlp],\n",
        "        \"Recall\": [\n",
        "            recall_score(y_val, y_val_pred_trivial),\n",
        "            recall_score(y_val, y_val_pred_lr),\n",
        "            recall_score(y_val, y_val_pred_mlp),\n",
        "        ],\n",
        "        \"Precision\": [\n",
        "            precision_score(y_val, y_val_pred_trivial, zero_division=0),\n",
        "            precision_score(y_val, y_val_pred_lr, zero_division=0),\n",
        "            precision_score(y_val, y_val_pred_mlp, zero_division=0),\n",
        "        ],\n",
        "        \"F1\": [\n",
        "            f1_score(y_val, y_val_pred_trivial),\n",
        "            f1_score(y_val, y_val_pred_lr),\n",
        "            f1_score(y_val, y_val_pred_mlp),\n",
        "        ],\n",
        "    }\n",
        ")\n",
        "comparison.style.format(\n",
        "    {\"PR AUC\": \"{:.4f}\", \"Recall\": \"{:.4f}\", \"Precision\": \"{:.4f}\", \"F1\": \"{:.4f}\"}\n",
        ").highlight_max(subset=[\"PR AUC\", \"Recall\", \"Precision\", \"F1\"], color=\"#d4edda\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---------------------------------------------------------------------------\n",
        "# PR curve overlay: all three models\n",
        "# ---------------------------------------------------------------------------\n",
        "fig, ax = plt.subplots(figsize=(7, 5))\n",
        "\n",
        "ax.plot(rec_t, prec_t, linestyle=\"--\", color=\"grey\", label=f\"Trivial (PR AUC={pr_auc_trivial:.4f})\")\n",
        "ax.plot(rec_lr, prec_lr, color=\"steelblue\", label=f\"Logistic Reg (PR AUC={pr_auc_lr:.4f})\")\n",
        "ax.plot(rec_mlp, prec_mlp, color=\"darkorange\", label=f\"Starting MLP (PR AUC={pr_auc_mlp:.4f})\")\n",
        "\n",
        "ax.set_xlabel(\"Recall\")\n",
        "ax.set_ylabel(\"Precision\")\n",
        "ax.set_title(\"Precision–Recall Curves (Validation Set)\")\n",
        "ax.legend(loc=\"upper right\")\n",
        "ax.set_xlim([0, 1.02])\n",
        "ax.set_ylim([0, 1.05])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.6 — Confusion matrix heatmaps\n",
        "\n",
        "The raw confusion matrices printed above are hard to scan quickly. The heatmaps below visualize them side by side so the relative magnitude of TP, FP, TN, and FN is immediately apparent. Darker cells mean larger counts. Annotations show the exact numbers inside each cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import seaborn as sns\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Confusion matrix heatmaps (side by side)\n",
        "# ---------------------------------------------------------------------------\n",
        "cms = {\n",
        "    \"Trivial Baseline\": confusion_matrix(y_val, y_val_pred_trivial),\n",
        "    \"Logistic Regression\": confusion_matrix(y_val, y_val_pred_lr),\n",
        "    \"Starting MLP\": confusion_matrix(y_val, y_val_pred_mlp),\n",
        "}\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
        "for ax, (title, cm_data) in zip(axes, cms.items()):\n",
        "    sns.heatmap(\n",
        "        cm_data,\n",
        "        annot=True,\n",
        "        fmt=\"d\",\n",
        "        cmap=\"Blues\",\n",
        "        xticklabels=[\"Non-fraud\", \"Fraud\"],\n",
        "        yticklabels=[\"Non-fraud\", \"Fraud\"],\n",
        "        ax=ax,\n",
        "        cbar=False,\n",
        "    )\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel(\"Predicted\")\n",
        "    ax.set_ylabel(\"Actual\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.7 — Predicted score distributions\n",
        "\n",
        "A good fraud detector should assign **high probabilities to fraud** and **low probabilities to non-fraud**, creating clear separation between the two distributions. If the distributions overlap heavily, the model cannot reliably distinguish fraud from legitimate transactions at any threshold.\n",
        "\n",
        "The histograms below show the distribution of predicted fraud probabilities for each model, split by the true label (blue = non-fraud, red = fraud). The vertical dashed line marks the 0.5 threshold used for hard predictions above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---------------------------------------------------------------------------\n",
        "# Predicted probability distributions: fraud vs non-fraud\n",
        "# ---------------------------------------------------------------------------\n",
        "score_sets = {\n",
        "    \"Logistic Regression\": y_val_prob_lr,\n",
        "    \"Starting MLP\": y_val_prob_mlp,\n",
        "}\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 4), sharey=False)\n",
        "\n",
        "for ax, (title, scores) in zip(axes, score_sets.items()):\n",
        "    mask_fraud = y_val == 1\n",
        "    mask_legit = y_val == 0\n",
        "\n",
        "    ax.hist(scores[mask_legit], bins=50, alpha=0.6, color=\"steelblue\",\n",
        "            label=f\"Non-fraud (n={mask_legit.sum()})\", density=True)\n",
        "    ax.hist(scores[mask_fraud], bins=50, alpha=0.7, color=\"crimson\",\n",
        "            label=f\"Fraud (n={mask_fraud.sum()})\", density=True)\n",
        "    ax.axvline(0.5, color=\"black\", linestyle=\"--\", linewidth=1, label=\"threshold = 0.5\")\n",
        "    ax.set_title(f\"{title} — Score Distribution\")\n",
        "    ax.set_xlabel(\"Predicted P(fraud)\")\n",
        "    ax.set_ylabel(\"Density\")\n",
        "    ax.legend(fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print summary statistics for fraud scores\n",
        "for title, scores in score_sets.items():\n",
        "    fraud_scores = scores[y_val == 1]\n",
        "    legit_scores = scores[y_val == 0]\n",
        "    print(f\"\\n{title}:\")\n",
        "    print(f\"  Fraud scores  — mean: {fraud_scores.mean():.4f}, median: {np.median(fraud_scores):.4f}, \"\n",
        "          f\"min: {fraud_scores.min():.4f}, max: {fraud_scores.max():.4f}\")\n",
        "    print(f\"  Legit scores  — mean: {legit_scores.mean():.4f}, median: {np.median(legit_scores):.4f}, \"\n",
        "          f\"min: {legit_scores.min():.4f}, max: {legit_scores.max():.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5 — Summary and interpretation\n",
        "\n",
        "**What we built:**\n",
        "\n",
        "| Model | Type | Purpose |\n",
        "|---|---|---|\n",
        "| Trivial baseline | Always predict class 0 | Absolute floor — any useful model must beat this |\n",
        "| Logistic regression | Linear classifier with balanced class weights | Shows how much signal lives in linear feature combinations |\n",
        "| Starting MLP | 3-layer feedforward neural network (128 → 64 → 32) with Dropout | First non-linear deep-learning model; captures feature interactions |\n",
        "\n",
        "#### Interpreting the results\n",
        "\n",
        "1. **Trivial baseline confirms accuracy is misleading.** A model that never flags fraud achieves ~99.87% accuracy but 0% recall — it misses every single fraud case. The confusion matrix heatmap makes this stark: the entire bottom row (actual fraud) is classified as non-fraud. This is why we use PR AUC, recall, and F1 as primary metrics.\n",
        "\n",
        "2. **Logistic regression provides a meaningful learned baseline.** With class weighting, it already catches a substantial fraction of fraud and provides calibrated probabilities. The score distribution histogram shows that logistic regression pushes most fraud cases toward higher probabilities, but there is still noticeable overlap with legitimate transactions, which explains why some fraud is missed and some legitimate transactions are flagged.\n",
        "\n",
        "3. **The MLP adds non-linear capacity.** The comparison table and PR curves reveal whether the three Dense layers and Dropout regularization improve ranking quality (PR AUC) and detection rate (recall) beyond the linear baseline. Compare the two score distribution histograms: if the MLP achieves cleaner separation (fraud scores concentrated near 1.0, legitimate scores near 0.0), it is learning useful non-linear patterns. If the improvement is marginal, it may suggest the fraud signal in this PCA-transformed dataset is largely linear.\n",
        "\n",
        "4. **Confusion matrix heatmaps make trade-offs visible.** For the learned models, look at the bottom-right cell (TP) versus the top-right cell (FP). A model with high TP but also high FP is aggressive — it catches fraud but creates false alarms. A model with low FP but low TP is conservative — it avoids false alarms but misses fraud. The right balance depends on the cost of each type of error.\n",
        "\n",
        "5. **Training curves confirm learning dynamics.** If the loss and AUC plots show train and validation curves converging and then the validation curve plateauing (with early stopping triggering), the model is neither severely underfitting nor overfitting. If there is a large gap between train and val curves, that signals overfitting — a problem that Step 7 will address through regularization tuning.\n",
        "\n",
        "6. **All evaluations use the validation set only.** The test set remains untouched and will only be used once in Step 8 for the final, unbiased performance report. This discipline prevents overfitting the evaluation.\n",
        "\n",
        "#### What comes next\n",
        "\n",
        "- **Step 6** will explore the model architecture in more detail and discuss design choices.\n",
        "- **Step 7** will tune the decision threshold (instead of using the default 0.5), experiment with hyperparameters, and run stability checks across multiple seeds.\n",
        "- **Step 8** will lock the final pipeline and report performance on the held-out test set.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6 — Develop the model\n",
        "\n",
        "### Overview of feedforward neural networks\n",
        "\n",
        "A feedforward network passes inputs through a stack of Dense layers without cycles. For tabular data, this is an effective first-choice architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Overview of multilayer perceptrons\n",
        "\n",
        "A multilayer perceptron (MLP) stacks Dense layers with nonlinear activations (e.g., ReLU) to learn complex decision boundaries from tabular numeric features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Justification: Why a feedforward MLP\n",
        "\n",
        "- Works well for fixed-length tabular numeric inputs.\n",
        "- Efficient to train and tune compared to more complex architectures.\n",
        "- Provides a strong baseline before exploring more specialized models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7 — Model improvement and threshold tuning\n",
        "\n",
        "- Instead of using a standard 0.5 cutoff, choose a decision threshold on the validation set (maximize recall subject to a minimum precision).\n",
        "- Use PR AUC as the primary model selection metric, which is more appropriate than accuracy for rare-event detection.\n",
        "- Run each configuration multiple times and report mean and standard deviation for stability.\n",
        "- Use a time-aware split to mirror real deployment conditions.\n",
        "- Apply binning-based calibration checks to verify probability quality.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8 — Final evaluation and deployment considerations\n",
        "\n",
        "### Current real-life applications\n",
        "\n",
        "Fraud detection systems are used by card issuers and payment networks to rank transactions by risk, trigger manual review, or apply step-up verification. In production, models are monitored for drift, recalibrated, and re-trained as fraud patterns evolve.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Glossary of terms\n",
        "\n",
        "\n",
        "| Term                  | Definition                                                                                                             |\n",
        "| --------------------- | ---------------------------------------------------------------------------------------------------------------------- |\n",
        "| Binary classification | A prediction task with two classes, here fraud versus non fraud                                                        |\n",
        "| Class imbalance       | A dataset property where one class is much rarer than the other                                                        |\n",
        "| Positive class        | The class of interest, here fraud transactions labeled 1                                                               |\n",
        "| Negative class        | The other class, here legitimate transactions labeled 0                                                                |\n",
        "| Feature               | An input variable used for prediction, such as V1 or Amount                                                            |\n",
        "| Label                 | The target variable the model learns to predict, here Class                                                            |\n",
        "| PCA                   | Principal Component Analysis, a transformation that creates new variables as linear combinations of original variables |\n",
        "| Principal component   | One PCA derived feature, here V1 to V28                                                                                |\n",
        "| Train set             | Data used to fit model parameters                                                                                      |\n",
        "| Validation set        | Data used to select hyperparameters and decision threshold                                                             |\n",
        "| Test set              | Held out data used once for final performance reporting                                                                |\n",
        "| Data leakage          | When information from validation or test data influences training or preprocessing decisions                           |\n",
        "| Standardization       | Scaling features to have zero mean and unit variance using training statistics                                         |\n",
        "| Normalization         | Rescaling features to a fixed range, often 0 to 1, depending on the method                                             |\n",
        "| Model                 | A function that maps input features to a predicted output                                                              |\n",
        "| Neural network        | A model composed of layers of learned transformations, here Dense and Dropout layers                                   |\n",
        "| Dense layer           | A fully connected layer that applies a linear transformation followed by an activation function                        |\n",
        "| Dropout               | A regularization method that randomly disables a fraction of units during training to reduce overfitting               |\n",
        "| Activation function   | A non linear function applied within a layer, such as ReLU or sigmoid                                                  |\n",
        "| Sigmoid               | An activation that maps a real number to a value between 0 and 1, used for binary outputs                              |\n",
        "| Logits                | The raw model output before applying sigmoid                                                                           |\n",
        "| Probability score     | The model output after sigmoid, interpreted as probability like score                                                  |\n",
        "| Decision threshold    | The cutoff used to convert probability scores into class predictions                                                   |\n",
        "| Confusion matrix      | A table counting true positives false positives true negatives and false negatives                                     |\n",
        "| True positive TP      | Fraud correctly predicted as fraud                                                                                     |\n",
        "| False positive FP     | Legitimate predicted as fraud                                                                                          |\n",
        "| True negative TN      | Legitimate correctly predicted as legitimate                                                                           |\n",
        "| False negative FN     | Fraud predicted as legitimate                                                                                          |\n",
        "| Precision             | TP divided by TP plus FP, the fraction of predicted fraud that is truly fraud                                          |\n",
        "| Recall                | TP divided by TP plus FN, the fraction of actual fraud that is detected                                                |\n",
        "| F1 score              | Harmonic mean of precision and recall                                                                                  |\n",
        "| ROC curve             | Curve of true positive rate versus false positive rate over thresholds                                                 |\n",
        "| AUC                   | Area under a curve, a threshold independent performance summary                                                        |\n",
        "| PR curve              | Precision versus recall over thresholds                                                                                |\n",
        "| PR AUC                | Area under the precision recall curve, often preferred for rare event detection                                        |\n",
        "| Overfitting           | When a model performs well on training but poorly on new data                                                          |\n",
        "| Regularization        | Methods that reduce overfitting, such as dropout or weight penalties                                                   |\n",
        "| Hyperparameter        | A setting chosen outside training, such as number of layers, dropout rate, learning rate                               |\n",
        "| Learning rate         | Step size used by the optimizer when updating model weights                                                            |\n",
        "| Optimizer             | Algorithm that updates model weights to minimize the loss, such as Adam                                                |\n",
        "| Loss function         | The quantity the model minimizes during training, such as binary cross entropy                                         |\n",
        "| Early stopping        | Stopping training when validation performance stops improving                                                          |\n",
        "| Calibration           | How well predicted probabilities match observed event rates                                                            |\n",
        "| Concept drift         | When the data generating process changes over time, causing performance degradation                                    |\n",
        "| Baseline model        | A simple reference model used for comparison, such as always predicting non fraud                                      |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### Bibliography & Citations\n",
        "\n",
        "\n",
        "Carcillo, F., Le Borgne, Y. A., Caelen, O. and Bontempi, G. (2018) ‘Streaming active learning strategies for real life credit card fraud detection: assessment and visualization’, International Journal of Data Science and Analytics, 5(4), pp. 285 300.\n",
        "\n",
        "Carcillo, F., Dal Pozzolo, A., Le Borgne, Y. A., Caelen, O., Mazzer, Y. and Bontempi, G. (2018) ‘Scarff: a scalable framework for streaming credit card fraud detection with Spark’, Information Fusion, 41, pp. 182 194.\n",
        "\n",
        "Carcillo, F., Le Borgne, Y. A., Caelen, O., Oblé, F. and Bontempi, G. (2019) ‘Combining unsupervised and supervised learning in credit card fraud detection’, Information Sciences.\n",
        "\n",
        "Dal Pozzolo, A. (n.d.) Adaptive machine learning for credit card fraud detection. PhD thesis. Université libre de Bruxelles.\n",
        "\n",
        "Dal Pozzolo, A., Caelen, O., Johnson, R. A. and Bontempi, G. (2015) ‘Calibrating probability with undersampling for unbalanced classification’, in Proceedings of the IEEE Symposium on Computational Intelligence and Data Mining. IEEE.\n",
        "\n",
        "Dal Pozzolo, A., Caelen, O., Le Borgne, Y. A., Waterschoot, S. and Bontempi, G. (2014) ‘Learned lessons in credit card fraud detection from a practitioner perspective’, Expert Systems with Applications, 41(10), pp. 4915 4928.\n",
        "\n",
        "Dal Pozzolo, A., Boracchi, G., Caelen, O., Alippi, C. and Bontempi, G. (2018) ‘Credit card fraud detection: a realistic modeling and a novel learning strategy’, IEEE Transactions on Neural Networks and Learning Systems, 29(8), pp. 3784 3797.\n",
        "\n",
        "Lebichot, B., Le Borgne, Y. A., He, L., Oblé, F. and Bontempi, G. (2019) ‘Deep learning domain adaptation techniques for credit cards fraud detection’, in INNSBDDL 2019 Recent Advances in Big Data and Deep Learning, pp. 78 88.\n",
        "\n",
        "Lebichot, B., Paldino, G., Siblini, W., He, L., Oblé, F. and Bontempi, G. (n.d.) ‘Incremental learning strategies for credit cards fraud detection’, International Journal of Data Science and Analytics.\n",
        "\n",
        "Le Borgne, Y. A. and Bontempi, G. (n.d.) Reproducible machine learning for credit card fraud detection: practical handbook.\n",
        "\n",
        "If you want, paste your target year for the PhD thesis and the handbook, plus any missing page ranges, and I will update the entries so everything is fully complete and consistent.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}