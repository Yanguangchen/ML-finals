{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Final Assignment\n",
        "## CM3015 Machine Learning and Neural Networks\n",
        "\n",
        "### Credit Card Fraud Detection with a Feedforward MLP\n",
        "\n",
        "- Student: cy150\n",
        "- Workflow: Chollet's ML workflow (problem → data → evaluation → prep → baseline → model → tuning → final eval)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1 — Define the problem\n",
        "\n",
        "Chollet’s workflow keeps the project aligned with real-world goals: define the problem and success metrics, understand the data, choose an evaluation protocol, prepare the data, establish a baseline, develop a first model, tune and improve it, then report final test results and deployment considerations.\n",
        "\n",
        "### Overview: Credit Card Fraud\n",
        "\n",
        "Credit card fraud is the unauthorized use of a credit (or debit) card to make purchases, withdraw funds, or create transactions that the legitimate cardholder did not approve."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem Statement\n",
        "\n",
        "Credit card fraud causes direct and significant financial losses: issuers, merchants, and consumers may absorb losses from unauthorized purchases and chargebacks. Fraud also creates investigative overhead (reviews, disputes) and temporary loss of funds and reputation.\n",
        "\n",
        "As a result, stricter verification and KYC are implemented by merchants and banks. These organizations can also lose customers if there are excessive false declines or chargebacks. Additionally, failure to protect customers according to regulatory standards can incur penalties.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2 — Identify and understand the data\n",
        "\n",
        "### Dataset Overview\n",
        "This dataset consists of credit card transactions by European cardholders. It covers two days of transactions with 492 frauds out of 284,807 transactions.\n",
        "The dataset is highly imbalanced, with frauds accounting for about 0.172% of all transactions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Nature of the Dataset\n",
        "\n",
        "It contains only numerical input variables resulting from a PCA transformation. Due to confidentiality, the original features and more background information are not available.\n",
        "\n",
        "### Dataset Features\n",
        "Features V1, V2, … V28 are principal components from PCA. The only features not transformed with PCA are `Time` and `Amount`.\n",
        "\n",
        "- `Time` contains the seconds elapsed between each transaction and the first transaction in the dataset.\n",
        "\n",
        "*Example*\n",
        "\n",
        "| Time |      V1 |      V2 |     V3 |     V4 |      V5 |\n",
        "| ---: | ------: | ------: | -----: | -----: | ------: |\n",
        "|    0 | -1.3598 | -0.0728 | 2.5363 | 1.3782 | -0.3383 |\n",
        "|    1 |  1.1919 |  0.2662 | 0.1665 | 0.4482 |  0.0600 |\n",
        "|    1 | -1.3584 | -1.3402 | 1.7732 | 0.3798 | -0.5032 |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset Licensing\n",
        "\n",
        "The dataset is licensed under the Database Contents License (DbCL).\n",
        "\n",
        "According to the license (Open Data Commons), the Licensor grants a worldwide, royalty-free, non-exclusive, perpetual, irrevocable copyright license to do any act that is restricted by copyright over anything within the Contents, whether in the original medium or any other. These rights explicitly include commercial use and do not exclude any field of endeavor.\n",
        "\n",
        "### Permission\n",
        "\n",
        "The DbCL license explicitly allows use of this dataset for this final assignment.\n",
        "\n",
        "### Dataset Author\n",
        "\n",
        "- Machine Learning Group - ULB "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset Source\n",
        "\n",
        "After browsing Kaggle, I selected a dataset that is complex and challenging while providing rich features for the model to learn from.\n",
        "\n",
        "Link to dataset: `https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud/data`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Justification for this dataset\n",
        "\n",
        "This dataset supports a full end-to-end deep learning workflow: clear labels, numeric features, and a real-world class imbalance that demands careful evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Rationale behind why this dataset was chosen\n",
        "\n",
        "- Real-world fraud detection is a technically challenging binary classification task that fits a feedforward MLP well.\n",
        "- The data are numeric and tabular with fixed-length features, aligning cleanly with Dense and Dropout layers.\n",
        "- Severe class imbalance pushes robust evaluation (precision, recall, PR AUC, threshold selection).\n",
        "\n",
        "#### Limitations of this dataset and mitigation strategy\n",
        "\n",
        "- Features V1 to V28 are anonymized PCA components while `Time` and `Amount` are not PCA transformed, limiting feature-level interpretation. I will treat them as informative numeric inputs and focus on predictive performance rather than interpretability.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1 Cont'd — Define success metrics\n",
        "\n",
        "### Objective\n",
        "\n",
        "The primary objective is to detect fraudulent transactions while minimizing false positives. Because fraud is rare, the evaluation focuses on minority-class performance and selecting an operating point that reflects the cost of errors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3 — Choose an evaluation protocol\n",
        "\n",
        "### Holdout Protocol\n",
        "\n",
        "1. The data will be split into:\n",
        "\n",
        "    - Training set\n",
        "    - Validation set\n",
        "    - Test set\n",
        "\n",
        "2. Preprocessing decisions are fitted using the training set exclusively.\n",
        "3. The validation set is only used for model and threshold selection.\n",
        "4. Final performance is evaluated on the untouched test set.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluation metrics\n",
        "\n",
        "This section describes how performance will be measured based on the pre-defined success criteria."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Primary Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| Metric               | What it measures                                                       | Why it matters for fraud under heavy class imbalance                                                    |\n",
        "| -------------------- | ---------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------- |\n",
        "| Precision–Recall AUC | Area under the precision–recall curve across decision thresholds       | Strong overall summary metric when fraud is rare, more informative than accuracy                        |\n",
        "| Recall               | True positive rate, how many actual fraud cases are correctly detected | Directly captures missed fraud risk since low recall means more fraud slips through                     |\n",
        "| F1 Score             | Harmonic mean of precision and recall                                  | Useful single number when you want a balanced tradeoff between catching fraud and limiting false alarms |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Secondary Metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| Secondary metric                              | Description                                                                                                       | Purpose                                                                                        |\n",
        "| --------------------------------------------- | ----------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------- |\n",
        "| Confusion matrix at a predetermined threshold | Uses TP, FP, TN, FN to make tradeoffs explicit                                                                    | Shows performance at the chosen operating point and clarifies the cost of each type of mistake |\n",
        "| Error rates                                   | False negative rate and false positive rate to quantify misses and false alarms                                   | Measures miss risk versus false alarm burden in a comparable way                               |\n",
        "| Calibration check                             | Compare predicted probabilities with observed outcomes using a simple binning table to verify probability quality | Checks whether predicted risk scores align with real observed fraud rates                      |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Justification for evaluation metrics\n",
        "\n",
        "Fraud is a rare event, so a single metric can be misleading. A model may look strong on one metric while failing in practice. The primary metric provides a consistent rule for model comparison, while secondary metrics provide the context needed to interpret false-positive and false-negative tradeoffs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Implications of the evaluation metrics\n",
        "\n",
        "1. Precision reflects workload and customer friction.\n",
        "2. Recall reflects loss prevention, often translating into direct financial losses.\n",
        "3. PR AUC reflects ranking quality under rare fraud across thresholds.\n",
        "4. The confusion matrix is reported at an operating threshold aligned with transaction behavior.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4 — Prepare the data\n",
        "\n",
        "### Data preparation plan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To prepare the data and avoid leakage:\n",
        "\n",
        "1. Use a time-aware split: earlier transactions for training, later transactions for validation/test.\n",
        "2. Fit all preprocessing steps (scaling, imputation if needed) on the training set only.\n",
        "3. Apply the same fitted transforms to validation and test sets.\n",
        "4. Preserve the class imbalance during splitting to reflect real deployment.\n",
        "5. Track feature distributions and label rate over time to identify drift.\n",
        "6. Use a small threshold sweep on the validation set for later operating-point selection.\n",
        "7. Calibrate probabilities with simple binning to sanity-check outputs.\n",
        "8. Re-train periodically as base rates drift in production."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4 (continued) — Data quality checks and class imbalance\n",
        "\n",
        "- Check for missing values, extreme outliers, and distribution shifts between splits.\n",
        "- Standardize numeric features using training-set statistics only.\n",
        "- Consider class-weighting or resampling strategies as part of the training plan.\n",
        "- Keep an audit trail of preprocessing decisions to support reproducibility.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5 — Establish a baseline and pick a starting model\n",
        "\n",
        "A trivial baseline is to always predict non-fraud; this sets the minimum acceptable recall and PR AUC. After establishing the baseline, I start with a small-to-medium feedforward MLP: fully connected Dense layers with Dropout and a single sigmoid output for binary classification.\n",
        "\n",
        "#### Model Size\n",
        "- Small to medium feedforward multilayer perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6 — Develop the model\n",
        "\n",
        "### Overview of feedforward neural networks\n",
        "\n",
        "A feedforward network passes inputs through a stack of Dense layers without cycles. For tabular data, this is an effective first-choice architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Overview of multilayer perceptrons\n",
        "\n",
        "A multilayer perceptron (MLP) stacks Dense layers with nonlinear activations (e.g., ReLU) to learn complex decision boundaries from tabular numeric features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Justification: Why a feedforward MLP\n",
        "\n",
        "- Works well for fixed-length tabular numeric inputs.\n",
        "- Efficient to train and tune compared to more complex architectures.\n",
        "- Provides a strong baseline before exploring more specialized models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7 — Model improvement and threshold tuning\n",
        "\n",
        "- Instead of using a standard 0.5 cutoff, choose a decision threshold on the validation set (maximize recall subject to a minimum precision).\n",
        "- Use PR AUC as the primary model selection metric, which is more appropriate than accuracy for rare-event detection.\n",
        "- Run each configuration multiple times and report mean and standard deviation for stability.\n",
        "- Use a time-aware split to mirror real deployment conditions.\n",
        "- Apply binning-based calibration checks to verify probability quality.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8 — Final evaluation and deployment considerations\n",
        "\n",
        "### Current real-life applications\n",
        "\n",
        "Fraud detection systems are used by card issuers and payment networks to rank transactions by risk, trigger manual review, or apply step-up verification. In production, models are monitored for drift, recalibrated, and re-trained as fraud patterns evolve.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Glossary of terms\n",
        "\n",
        "\n",
        "| Term                  | Definition                                                                                                             |\n",
        "| --------------------- | ---------------------------------------------------------------------------------------------------------------------- |\n",
        "| Binary classification | A prediction task with two classes, here fraud versus non fraud                                                        |\n",
        "| Class imbalance       | A dataset property where one class is much rarer than the other                                                        |\n",
        "| Positive class        | The class of interest, here fraud transactions labeled 1                                                               |\n",
        "| Negative class        | The other class, here legitimate transactions labeled 0                                                                |\n",
        "| Feature               | An input variable used for prediction, such as V1 or Amount                                                            |\n",
        "| Label                 | The target variable the model learns to predict, here Class                                                            |\n",
        "| PCA                   | Principal Component Analysis, a transformation that creates new variables as linear combinations of original variables |\n",
        "| Principal component   | One PCA derived feature, here V1 to V28                                                                                |\n",
        "| Train set             | Data used to fit model parameters                                                                                      |\n",
        "| Validation set        | Data used to select hyperparameters and decision threshold                                                             |\n",
        "| Test set              | Held out data used once for final performance reporting                                                                |\n",
        "| Data leakage          | When information from validation or test data influences training or preprocessing decisions                           |\n",
        "| Standardization       | Scaling features to have zero mean and unit variance using training statistics                                         |\n",
        "| Normalization         | Rescaling features to a fixed range, often 0 to 1, depending on the method                                             |\n",
        "| Model                 | A function that maps input features to a predicted output                                                              |\n",
        "| Neural network        | A model composed of layers of learned transformations, here Dense and Dropout layers                                   |\n",
        "| Dense layer           | A fully connected layer that applies a linear transformation followed by an activation function                        |\n",
        "| Dropout               | A regularization method that randomly disables a fraction of units during training to reduce overfitting               |\n",
        "| Activation function   | A non linear function applied within a layer, such as ReLU or sigmoid                                                  |\n",
        "| Sigmoid               | An activation that maps a real number to a value between 0 and 1, used for binary outputs                              |\n",
        "| Logits                | The raw model output before applying sigmoid                                                                           |\n",
        "| Probability score     | The model output after sigmoid, interpreted as probability like score                                                  |\n",
        "| Decision threshold    | The cutoff used to convert probability scores into class predictions                                                   |\n",
        "| Confusion matrix      | A table counting true positives false positives true negatives and false negatives                                     |\n",
        "| True positive TP      | Fraud correctly predicted as fraud                                                                                     |\n",
        "| False positive FP     | Legitimate predicted as fraud                                                                                          |\n",
        "| True negative TN      | Legitimate correctly predicted as legitimate                                                                           |\n",
        "| False negative FN     | Fraud predicted as legitimate                                                                                          |\n",
        "| Precision             | TP divided by TP plus FP, the fraction of predicted fraud that is truly fraud                                          |\n",
        "| Recall                | TP divided by TP plus FN, the fraction of actual fraud that is detected                                                |\n",
        "| F1 score              | Harmonic mean of precision and recall                                                                                  |\n",
        "| ROC curve             | Curve of true positive rate versus false positive rate over thresholds                                                 |\n",
        "| AUC                   | Area under a curve, a threshold independent performance summary                                                        |\n",
        "| PR curve              | Precision versus recall over thresholds                                                                                |\n",
        "| PR AUC                | Area under the precision recall curve, often preferred for rare event detection                                        |\n",
        "| Overfitting           | When a model performs well on training but poorly on new data                                                          |\n",
        "| Regularization        | Methods that reduce overfitting, such as dropout or weight penalties                                                   |\n",
        "| Hyperparameter        | A setting chosen outside training, such as number of layers, dropout rate, learning rate                               |\n",
        "| Learning rate         | Step size used by the optimizer when updating model weights                                                            |\n",
        "| Optimizer             | Algorithm that updates model weights to minimize the loss, such as Adam                                                |\n",
        "| Loss function         | The quantity the model minimizes during training, such as binary cross entropy                                         |\n",
        "| Early stopping        | Stopping training when validation performance stops improving                                                          |\n",
        "| Calibration           | How well predicted probabilities match observed event rates                                                            |\n",
        "| Concept drift         | When the data generating process changes over time, causing performance degradation                                    |\n",
        "| Baseline model        | A simple reference model used for comparison, such as always predicting non fraud                                      |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### Bibliography & Citations\n",
        "\n",
        "\n",
        "Carcillo, F., Le Borgne, Y. A., Caelen, O. and Bontempi, G. (2018) ‘Streaming active learning strategies for real life credit card fraud detection: assessment and visualization’, International Journal of Data Science and Analytics, 5(4), pp. 285 300.\n",
        "\n",
        "Carcillo, F., Dal Pozzolo, A., Le Borgne, Y. A., Caelen, O., Mazzer, Y. and Bontempi, G. (2018) ‘Scarff: a scalable framework for streaming credit card fraud detection with Spark’, Information Fusion, 41, pp. 182 194.\n",
        "\n",
        "Carcillo, F., Le Borgne, Y. A., Caelen, O., Oblé, F. and Bontempi, G. (2019) ‘Combining unsupervised and supervised learning in credit card fraud detection’, Information Sciences.\n",
        "\n",
        "Dal Pozzolo, A. (n.d.) Adaptive machine learning for credit card fraud detection. PhD thesis. Université libre de Bruxelles.\n",
        "\n",
        "Dal Pozzolo, A., Caelen, O., Johnson, R. A. and Bontempi, G. (2015) ‘Calibrating probability with undersampling for unbalanced classification’, in Proceedings of the IEEE Symposium on Computational Intelligence and Data Mining. IEEE.\n",
        "\n",
        "Dal Pozzolo, A., Caelen, O., Le Borgne, Y. A., Waterschoot, S. and Bontempi, G. (2014) ‘Learned lessons in credit card fraud detection from a practitioner perspective’, Expert Systems with Applications, 41(10), pp. 4915 4928.\n",
        "\n",
        "Dal Pozzolo, A., Boracchi, G., Caelen, O., Alippi, C. and Bontempi, G. (2018) ‘Credit card fraud detection: a realistic modeling and a novel learning strategy’, IEEE Transactions on Neural Networks and Learning Systems, 29(8), pp. 3784 3797.\n",
        "\n",
        "Lebichot, B., Le Borgne, Y. A., He, L., Oblé, F. and Bontempi, G. (2019) ‘Deep learning domain adaptation techniques for credit cards fraud detection’, in INNSBDDL 2019 Recent Advances in Big Data and Deep Learning, pp. 78 88.\n",
        "\n",
        "Lebichot, B., Paldino, G., Siblini, W., He, L., Oblé, F. and Bontempi, G. (n.d.) ‘Incremental learning strategies for credit cards fraud detection’, International Journal of Data Science and Analytics.\n",
        "\n",
        "Le Borgne, Y. A. and Bontempi, G. (n.d.) Reproducible machine learning for credit card fraud detection: practical handbook.\n",
        "\n",
        "If you want, paste your target year for the PhD thesis and the handbook, plus any missing page ranges, and I will update the entries so everything is fully complete and consistent.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
